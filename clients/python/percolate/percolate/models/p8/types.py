"""
all the table models are added here for p8 schema and can be iterated and used to generate install scripts
we add system fields in the sql model such as timestamps and user ids
"""

import uuid
from pydantic import Field, model_validator
from pydantic.fields import FieldInfo
import typing
from ..AbstractModel import AbstractModel, BaseModel, AbstractEntityModel
from .. import inspection
from percolate.utils import make_uuid
import datetime
from .. import DefaultEmbeddingField, KeyField
from percolate.utils.names import EmbeddingProviders
import percolate as p8
import json
from percolate.utils import get_iso_timestamp
import hashlib
from enum import Enum
from .db_types import AccessLevel
#we shouldnt really need a logger her but we have temp functions being exported to apis
from percolate.utils import logger 
class Function(AbstractEntityModel):
    """Functions are external tools that agents can use. See field comments for context.
    Functions can be searched and used as LLM tools. 
    The function spec is derived from OpenAPI but adapted to the conventional used in LLMs
    """
         
    id: uuid.UUID | str = Field(description="A unique id in this case generated by the proxy and function name") 
    key: typing.Optional[str] = Field(description='optional key e.g operation id')
    name: str = Field(description="A friendly name that is unique within the proxy scope e.g. a single api or python library")
    verb:  typing.Optional[str] = Field(None,description= "The verb e.g. get, post etc")
    endpoint: typing.Optional[str] = Field(None, description="A callable endpoint in the case of REST")
    description: str = DefaultEmbeddingField('', description="A detailed description of the function - may be more comprehensive than the one within the function spec - this is semantically searchable")
    function_spec: dict = Field(description="A function description that is OpenAI and based on the OpenAPI spec")
    proxy_uri: str = Field(description='a reference to an api or library namespace that qualifies the named function')
    
    def __call__(self, *args, **kwargs):
        """
        Convenience to call any function via its proxy
        """
        return p8.get_proxy(self.proxy_uri).invoke(self, **kwargs)
        
    @classmethod
    def from_entity(cls, model:BaseException):
        """The function produces a callable agent stub that can be searched.
           In the database we use a native generic function handler and in python we load the agent from the name to call the agent.
           Agents are discoverable in this way and we can transfer context to other agents.
        """
        M:AbstractModel = AbstractModel.Abstracted(model)
    
        description = f"""
        ## Agent: {M.get_model_full_name()}
        ## Description
        ```
        {M.get_model_description()}
        ```
        ## Functions
        ```json
        {M.get_model_functions()}
        ```
        """
        
        """this must be the same interface as the Agent interface i.e. we relay to the agent.run(question='')"""
        def run(question:str):
            """
            send any query or request to the agent to execute the agent
            
            Args:
                question: a request to the agent
            """
            pass
        
        key = f"{M.get_model_full_name()}.run".replace('.','_')
        
        """the alias is important because we need to qualify functions used by agents and the proxy above is just a signature hint"""
        spec = cls.from_callable(run,alias=key).function_spec
        """names should not have '.'"""
        
        """for the name we qualify with run agent so its clear what we are doing"""
        return Function(name=key, 
                        key=key,
                        description=description,
                        function_spec=spec,
                        verb='get',
                        proxy_uri=f'p8agent/{M.get_model_full_name()}', #handler can use this - a p8_agent/agent.name - see interface proxy manager
                        endpoint='run')
        
    @classmethod
    def from_callable(cls, fn:typing.Callable, remove_untyped:bool=True, proxy_uri:str=None,alias:str=None):
        """construct function from callable - supply an alias if the function is not full qualified or if we need extra context"""
        def process_properties(properties: dict):
             
            untyped = []
            """google at least wont like some of these but they are redundant anyway"""
            for key, details in properties.items():
                for remove_field in ['title', 'default']:
                    if remove_field in details:
                        details.pop(remove_field)
       
                if 'anyOf' in details:
                    new_list = [t for t in details['anyOf'] if t['type']!= 'null']
                   
                    #temp ignore anything after the first type
                    if len(new_list) >= 1:
                        details['type'] = new_list[0]['type']
                        details.pop('anyOf')
                    else:
                        details.pop('anyOf')
                        details['oneOf'] = new_list
               
                if 'properties' in details:
                    process_properties(details['properties'])
                
                """for language models there is no point sending untyped information
                TODO: handle this better e.g. typed kwargs - specifying additional properties true might be enough but its unclear how to handle them anyway
                """
                if not details and remove_untyped:
                    untyped.append(key)
                

            
        def _map(f):
            """make sure the structure from pydantic is the same as used elsewhere for functions"""
            p = dict(f)
            if 'properties' in p:
                process_properties(p['properties'])
    
            name = p.pop('title')
            desc = p.pop('description') if 'description' in p else 'NO DESC'
            return {
                'name': alias or name,
                'parameters' :p,
                'description': desc
            }
        
        """we can pass this in e.g. for native. Lib is used for lib runtime or APIs for REST"""
        if not proxy_uri:
            proxy_uri = fn.__self__ if hasattr(fn, '__self__') else 'lib'
            if not isinstance(proxy_uri,str):
                proxy_uri = inspection.get_object_id(proxy_uri)
            
        s = _map(AbstractModel.create_model_from_function(fn).model_json_schema())
        key = s['name'] if not proxy_uri else f"{proxy_uri}.{s['name']}"
        id_md5_uuid =  uuid.uuid3(uuid.NAMESPACE_DNS, key)   
        return cls(id=str(id_md5_uuid), 
                   name = s['name'],
                   key  = key, 
                   endpoint=s['name'],
                   verb='get',
                   proxy_uri=proxy_uri,
                   spec=s, 
                   function_spec=s,
                   description=s['description'],
                   #this is ignored by Function but the runtime function can carry it
                   fn=fn)
        
    @model_validator(mode='before')
    @classmethod
    def _f(cls, values):
        if not values.get('id'):
            values['id'] = make_uuid({ 'key': values['name'], 'proxy_uri':values['proxy_uri']})
        return values
        
   
class ApiProxy(AbstractEntityModel):
    """A list of proxies or APIs that have attached functions or endpoints - links to proxy_uri on the Function""" 
    id: typing.Optional[uuid.UUID | str] = Field(None, description="Will default to a hash of the uri")
    name: typing.Optional[str] = Field(None, description="A unique api friendly name")
    proxy_uri: str = Field(description='a reference to an api or library namespace that qualifies the named function')
    token: typing.Optional[str] = Field(None, description="the token to save")
    
            
    @model_validator(mode='before')
    @classmethod
    def _f(cls, values):
        if not values.get('name'):
            values['name'] = values['proxy_uri']
        if not values.get('id'):
            values['id'] = make_uuid(values['proxy_uri'])
        return values
        
    
    
class LanguageModelApi(AbstractEntityModel):
    """The Language model REST Apis are stored with tokens and scheme information.
    """
    
    id: uuid.UUID | str
    name: str = Field(description="A unique name for the model api e.g cerebras-llama3.1-8b")
    model: typing.Optional[str] = Field(None, description="The model name defaults to the name as they are often the same. the name can be unique based on a provider qualfier")
    scheme: typing.Optional[str] = Field('openai', description="In practice most LLM APIs use an openai scheme - currently `anthropic` and `google` can differ")
    completions_uri: str = Field(description="The api used for completions in chat and function calling. There may be other uris for other contexts")
    token_env_key: typing.Optional[str] = Field(None, description="Conventions are used to resolve keys from env or other services. Provide an alternative key")
    token: typing.Optional[str] = Field(None, description="It is not recommended to add tokens directly to the data but for convenience you might want to")
    
        
    @model_validator(mode='before')
    @classmethod
    def _f(cls, values):
        if not values.get('model'):
            values['model'] = values['name']
        return values
        
    
class Agent(AbstractEntityModel):
    """The agent model is a meta data object to persist agent metadata for search etc"""

    id: uuid.UUID| str  
    name: str
    category: typing.Optional[str] = Field(None,description="Simple property to filter agents by categories")
    description: str = DefaultEmbeddingField(description="The system prompt as markdown")
    spec: dict = Field(description="The model json schema")
    functions: typing.Optional[dict] = Field(description="The function that agent can call",default_factory=dict)
    metadata: typing.Optional[dict] = Field(description="Custom metadata added to the agent",default_factory=dict)

    @model_validator(mode='before')
    @classmethod
    def _f(cls, values):
        """we take these from the class and save them"""
        if not values.get('functions') and hasattr(cls, 'get_model_functions'):
            values['functions'] = cls.get_model_functions()
        return values
    
    
    def from_abstract_model(cls: BaseModel):
        """Given any pydantic model that behaves like an AbstractModel, get the meta object i.e. Agent"""
        cls:AbstractModel = AbstractModel.Abstracted(cls)
        name = cls.get_model_full_name()
        return Agent(id=make_uuid(name), 
                     name=name, 
                     functions=cls.get_model_functions(),
                     spec = cls.model_json_schema(),
                     description=cls.get_model_description())
    
    @classmethod
    def _create_model_from_data(cls, agent_data: dict) -> AbstractModel:
        """
        Create a model from agent data dictionary
        
        Args:
            agent_data: Dictionary with agent data (as returned by repository)
            
        Returns:
            A dynamically created model with all fields, config, and functions
        """
        # Extract namespace and name from the full name
        if '.' in agent_data['name']:
            namespace, model_name = agent_data['name'].rsplit('.', 1)
        else:
            namespace = 'public'
            model_name = agent_data['name']
        
        # Convert the JSON Schema spec to Pydantic fields
        fields = AbstractModel.fields_from_json_schema(agent_data['spec'])
        
        # Create the model with fields
        model = AbstractModel.create_model(
            name=model_name,
            namespace=namespace,
            description=agent_data.get('description', ''),
            functions=agent_data.get('functions'),
            fields=fields,
            inherit_config=False  # Don't inherit AbstractModel's config
        )
        
        # Update model_config with metadata if present
        if agent_data.get('metadata'):
            if hasattr(model, 'model_config') and isinstance(model.model_config, dict):
                model.model_config.update(agent_data['metadata'])
            else:
                # If model_config doesn't exist or isn't a dict, create it
                model.model_config = {
                    'name': model_name,
                    'namespace': namespace,
                    'description': agent_data.get('description', ''),
                    'functions': agent_data.get('functions'),
                    **agent_data['metadata']
                }
        
        # Store the original agent ID for reference
        model.model_config['agent_id'] = str(agent_data['id'])
        
        return model
    
    @classmethod
    def load(cls, name: str) -> AbstractModel:
        """
        Load an agent from the database and reconstruct it as a proper model
        
        Args:
            name: The agent name (can be namespace.name or just name)
            
        Returns:
            A dynamically created model with all fields, config, and functions
            
        Raises:
            ValueError: If agent not found or multiple agents found
        """
        try:
            # Query the database for the agent
            agents = p8.repository(Agent).select(name=name)
            
            if not agents:
                raise ValueError(f"Agent '{name}' not found in database")
            if len(agents) > 1:
                raise ValueError(f"Multiple agents found with name '{name}'. Please use fully qualified name.")
            
            # Repository returns dicts, not objects
            agent_data = agents[0]
            
            return cls._create_model_from_data(agent_data)
            
        except Exception as e:
            logger.error(f"Failed to load agent '{name}': {str(e)}")
            raise

class ModelField(AbstractEntityModel):
    """Fields are each field in any saved model/agent. 
    Fields are useful for describing system info such as for embeddings or for promoting.
    """
            
    id: typing.Optional[uuid.UUID|str] = Field(description="a unique key for the field e.g. field and entity key hashed")
    name: str = Field(description="The field name")
    entity_name: str
    field_type: str
    embedding_provider: typing.Optional[str] = Field(None, description="The embedding could be a multiple in future")
    description: typing.Optional[str] = None
    is_key: typing.Optional[bool] = Field(default=False, description="Indicate that the field is the primary key - our convention is the id field should be the primary key and be uuid and we use this to join embeddings")
    
    @model_validator(mode='before')
    @classmethod
    def _f(cls, values):
        """this is a convenience for 'dont care' cases where you want an embedding but dont know the model name"""
        if values.get('embedding_provider') == 'default':
            values['embedding_provider'] = EmbeddingProviders.embedding_text_embedding_ada_002
        if not values.get('id'):
            values['id'] = make_uuid({'name': values['name'], 'entity_name': values['entity_name']})
        return values
    
    @classmethod
    def from_field_info(cls, key:str, field_type: FieldInfo, parent_model:BaseModel):
        """five a FieldInfo object map the field"""
        
        parent_model:AbstractModel = AbstractModel.Abstracted(parent_model)
        json_args = field_type.json_schema_extra or {}
        arg = inspection.get_innermost_args(field_type.annotation)
        arg = str(arg.__name__) if hasattr(arg, '__name__') else str(arg)
        return ModelField(name=key, 
                           entity_name= parent_model.get_model_full_name(), 
                           field_type= arg,
                           description=field_type.description,
                           **json_args)
        
    @staticmethod
    def get_fields_from_model(cls:BaseModel):
        """for any pydantic model generate the field collection"""
        cls = AbstractModel.Abstracted(cls)
        return [ModelField.from_field_info(key, value,parent_model=cls) for key, value in cls.model_fields.items()]
    
 

class TokenUsage(AbstractModel):
    """Tracks token usage for language model interactions"""
    model_config = {'protected_namespaces': ()}
    id: uuid.UUID| str  
    model_name: str
    tokens: typing.Optional[int] = Field(0,description="the number of tokens consumed in total")
    tokens_in: typing.Optional[int] = Field(0, description="the number of tokens consumed for input")
    tokens_out: typing.Optional[int] = Field(0, description="the number of tokens consumed for output")
    tokens_other: typing.Optional[int] = Field(0, description="the number of tokens consumed for functions and other metadata")
    session_id: typing.Optional[uuid.UUID| str  ] = Field(None, description="Session id for a conversation")
    
    @model_validator(mode='before')
    @classmethod
    def _f(cls, values):
        if not values.get('tokens'):
            values['tokens'] = (values.get('tokens_in') or 0) + (values.get('tokens_out') or 0)
        return values
    
class IndexAudit(TokenUsage):
    model_config = {
        'index_notify': False        
    }
    """Track requests to build smart indexes such as graph links or text and image embeddings"""
    metrics: typing.Optional[dict] = Field(description="metrics for records indexed",default_factory=dict)
    status: str = Field(description="Status code such as OK|ERROR")
    message: typing.Optional[str] = Field(description="Any message such as an error")
    entity_full_name: str
    
    
class _OpenAIMessage(BaseModel):
    role: str
    content: typing.Optional[str] = Field('', description='text content')
    tool_calls: typing.Optional[typing.List[dict]|dict]
    tool_call_id: typing.Optional[str] = None
    
    @classmethod
    def from_message(cls, values):
        """
        
        """
        return _OpenAIMessage(**values, tool_call_id=(values.get('tool_eval_data') or {}).get('id'))
    
    @model_validator(mode='before')
    @classmethod
    def _val(cls,values):
        if tool_calls:= values.get('tool_calls'):
            if isinstance(tool_calls,dict):
                values['tool_calls'] = [tool_calls]
        return values

class AIResponse(TokenUsage):
    """Each atom in an exchange between users, agents, assistants and so on. 
    We generate questions with sessions and then that triggers an exchange. 
    Normally the Dialogue is round trip transaction.
    """
    model_config = {
        'protected_namespaces': (),
        'access_level': AccessLevel.ADMIN  # Admin access level
    }
    id: uuid.UUID| str  
    role: str = Field(description="The role of the user/agent in the conversation")
    content: str = DefaultEmbeddingField(description="The content for this part of the conversation") #TODO we may not want to automatically generate embeddings for this table
    status: typing.Optional[str] = Field(description="The status of the session such as REQUEST|RESPONSE|ERROR|TOOL_CALL|STREAM_RESPONSE")
    tool_calls: typing.Optional[typing.List[dict]|dict] = Field(default=None, description="Tool calls are requests from language models to call tools")
    tool_eval_data: typing.Optional[dict] = Field(default=None, description="The payload may store the eval from the tool especially if it is small data")
    verbatim: typing.Optional[dict|typing.List[dict]] = Field(default=None, description="the verbatim message from the language model - we dont serialize this", exclude=True)     
    function_stack: typing.Optional[typing.List[str]] = Field(None, description='At each stage certain functions are available to the model - useful to see what it has and what it chooses and to reload stack later')
 
    def to_open_ai_message(self):
        """the message structure for the scheme
        the one thing we need to do is make sure that tool calls reference the id(s)
        """
        return _OpenAIMessage.from_message(self.model_dump())
    
    @classmethod
    def from_open_ai_response(cls, response, sid:str)  :
        """"""
        choice = response['choices'][0]
        usage = response['usage']
        message = choice['message']
        tool_calls= message.get('tool_calls')
        return cls(
            id=str(uuid.uuid1()),
            role=message.get('role'),
            tool_calls=tool_calls,
            content=message.get('content') or '',
            tokens_in=usage['prompt_tokens'],
            tokens_out=usage['completion_tokens'],
            model_name=response['model'],
            status='TOOL_CALL' if not tool_calls else 'RESPONSE',
            session_id=sid
        )
class Session(AbstractModel):
    """Tracks groups if session dialogue - sessions are requests that produce actions or activity. 
    Typical example is interacting with an AI but also it could be simply a request to bookmark or ingestion some content.
    From the user's perspective a session is a single interaction but internally the system performs multiple hops in general - hence "session"
    Sessions can be interactions with the system or agent but they can be inferred e.g. from user activity
    Sessions can also be abstract for example a daily diary can be an abstract session with the intent of capturing daily activity
    Sessions can be hierarchical and moments can be generated from sessions.
    
    Sessions model user intent.
    """
    model_config = {
        'access_level': AccessLevel.ADMIN  # Admin access level
    }
    id: uuid.UUID| str  
    """i should maybe deprecate the name here as this is a dense audit but then maybe we should just archive?"""
    name: typing.Optional[str] = Field(None, description="The name is a pseudo name to make sessions node-compatible")
    query: typing.Optional[str] = DefaultEmbeddingField(None,description='the question or context that triggered the session - query can be an intent and it can be inferred')
    user_rating: typing.Optional[float] = Field(None, description="We can in future rate sessions to learn what works")
    agent: str = Field('percolate', description="Percolate always expects an agent but we support passing a system prompt which we treat as anonymous agent")
    parent_session_id:typing.Optional[uuid.UUID| str] = Field(None, description="A session is a thread from a question+prompt to completion. We span child sessions")
    
    thread_id: typing.Optional[str] = Field(None,description='An id for a thread which can contain a number of sessions - thread matches are case insensitive ie.. MyID==myid - typically we prefer ids to be uuids but depends on the system')
    channel_id: typing.Optional[str] = Field(None, description="The platform/channel ID through which the user is interacting (e.g., specific Slack channel ID)")
    channel_type: typing.Optional[str] = Field('percolate', description="The platform type (e.g., 'slack', 'percolate', 'email') - the device info + the channel type tells if its mobile etc")
    session_type: typing.Optional[str] = Field('conversation', description="The type of session (e.g., 'conversation', 'resource_management', 'task_creation', 'daily_digest')")

    metadata: typing.Optional[dict] = Field(default_factory=dict, description="Arbitrary metadata - typically this is device info and other context")
    session_completed_at: typing.Optional[datetime.datetime] = Field(default=None,description="An audit timestamp to write back the completion of the session - its optional as it should manage the last timestamp on the AI Responses for the session")
    graph_paths: typing.Optional[typing.List[str]] = Field(None, description="Track all paths extracted by an agent as used to build the KG over user sessions")
    userid:  typing.Optional[uuid.UUID| str ] = Field(None,description="The user id to use") #implicit
        
    
    @model_validator(mode='before')
    @classmethod
    def _f(cls, values): 
        if isinstance(values,dict) and not values.get('id'):
            """the name is mapped for a hash of the query and can create collisions for now since we only want to index intent"""
            # TODO - consider if this is ok ^
            values['name'] = make_uuid({'query': values.get('query', '')})
        return values
    
    @classmethod
    def get_daily_diary_thread_id(cls, userid:str,  session_type:str=None):
        """this is a simple hash for filtering and respect the contract for threads"""
        daily_diary_id = make_uuid({"user_id": userid, "session_type": session_type})
        return daily_diary_id
    
    @classmethod
    def daily_diary_entry(cls, userid:str, query:str, metadata:dict=None, session_type:str='daily_digest'):
        """
        generates a task for the daily diary
        the query should be the intent e.g. a question from a user or an action such as upload file
        """
        
        return Session(id = str(uuid.uuid1()), 
                       query=query,
                       name =f"Diary entry {get_iso_timestamp()}",
                       session_type=session_type, 
                       userid=userid,
                       thread_id=cls.get_daily_diary_thread_id(userid=userid,session_type=session_type), 
                       metadata=metadata)
    
    @classmethod
    def task_thread_entry(cls, thread_id :str,  userid:str, query:str, metadata:dict=None,session_type:str='daily_digest'):
        """
        generates a task for the daily diary
        the query should be the intent e.g. a question from a user or an action such as upload file
        """
        
        return Session(id = str(uuid.uuid1()), 
                       query=query,
                       name =f"Diary entry {get_iso_timestamp()}",
                       session_type=session_type, 
                       userid=userid,
                       thread_id=thread_id, 
                       metadata=metadata)
    
    
    
    
    @classmethod
    def from_question_and_context(cls, id:str, question:str, context:typing.Any, agent:str=None, **kwargs):
        """
        from the session we can track the question and the agent that serviced. we may also want to store other details 
        """
        from percolate.services.llm import CallingContext
        context: CallingContext = context
        
        """support alias for slack only models"""
        channel_id = getattr(context, 'channel_id', None) or  getattr(context, 'channel_context', None)
        thread_id = getattr(context, 'thread_id', None) or  getattr(context, 'channel_ts', None)
        
        """the email can a slack user so contract is to also supply user id in that context
        the only legal way to not supply an id is to supply the username from which the id is hashed
        """
        user_name = context.username
        user_id = context.user_id
        if not user_id:
            user_id = make_uuid(user_name) if user_name else None
        # use name below assumed to tbe users email address which is unique
        return cls(query=question,
            id=id or str(uuid.uuid1()), 
            agent=agent,
            userid = user_id,
            channel_id=channel_id,
            thread_id=thread_id,
            **kwargs)
         
class SessionEvaluation(AbstractModel):
    """Tracks groups if session dialogue"""
    id: uuid.UUID| str  
    rating: float = Field(None,description="A rating from 0 to 1 - binary thumb-up/thumbs-down are 0 or 1")
    comments: typing.Optional[str] = Field(None, description="Additional feedback comments from the user")
    session_id: uuid.UUID| str  
    
class ModelMatrix(AbstractModel):
    """keep useful json blobs for model info"""
    id: uuid.UUID| str  
    entity_name: str = Field(description="The name of the entity e.g. a model in the types or a user defined model")
    enums: typing.Optional[dict] = Field(None, description="The enums used in the model - usually a job will build these and they can be used in query prompts")
    example_sql_queries: typing.Optional[dict] = Field(None, description="A mapping of interesting question to SQL queries - usually a job will build these and they can be used in query prompts")

class Category(AbstractEntityModel):
    """A category is just a general topic node that can be linked to other nodes and can have an evolving description or summary"""
    id: uuid.UUID| str  
    name: str = Field(description="The name of the category node")
    description: str = DefaultEmbeddingField(description="The content description for the category")
    
class Project(AbstractEntityModel):
    """A project is a broadly defined goal with related resources (uses the graph)"""
    id: uuid.UUID| str  
    name: str = Field(description="The name of the entity e.g. a model in the types or a user defined model")
    description: str = DefaultEmbeddingField(description="The content for this part of the conversation")
    target_date: typing.Optional[datetime.datetime] = Field(None, description="Optional target date")
    collaborator_ids: typing.List[uuid.UUID] = Field(default_factory=list, description="Users collaborating on this project")
    status: typing.Optional[str] = Field(default="active", description="Project status")
    priority: typing.Optional[int] = Field(default=1, description="Priority level (1-5), 1 being the highest priority")
    
class Task(Project):
    """Tasks are sub projects. A project can describe a larger objective and be broken down into tasks.
If if you need to do research or you are asked to search or create a research iteration/plan for world knowledge searches you MUST ask the _ResearchIteration agent_ to help perform the research on your behalf (you can request this agent with the help i.e. ask for an agent by name). 
You should be clear when relaying the users request. If the user asks to construct a plan, you should ask the agent to construct a plan. If the user asks to search, you should ask the research agent to execute  a web search.
If the user asks to look for existing plans, you should ask the research agent to search research plans.
    """
    id: typing.Optional[uuid.UUID| str] = Field(None,description= 'id generated for the name and project - these must be unique or they are overwritten')
    project_name: typing.Optional[str] = Field(None, description="The related project name if relevant")
    estimated_effort: typing.Optional[float] = Field(default=None, description="Estimated effort in hours")
    progress: typing.Optional[float] = Field(default=0.0, description="Completion progress (0-1)")
    
    @model_validator(mode='before')
    @classmethod
    def _f(cls, values): 
        if isinstance(values,dict) and not values.get('id'):
            values['id'] = make_uuid({'name': values['name'], 'project_name': values.get('project_name') or ''})
        return values
    
    @classmethod
    def get_model_functions(cls):
        """fetch task external functions"""
        return {'post_tasks_': 'Used to save tasks by posting the task object. Its good practice to first search for a task of a similar name before saving in case of duplicates' }
    
class User(AbstractEntityModel):
    """A model of a logged in user"""
    model_config = {
        'access_level': AccessLevel.ADMIN  # Admin access
    }
    id: uuid.UUID| str  
    name: typing.Optional[str] = Field(None,description="A display name for a user")
    email: typing.Optional[str] = KeyField(None,description="email of the user if we know it - the key must be something unique like this")
    slack_id: typing.Optional[str] = Field(None,description="slack user U12345")
    linkedin: typing.Optional[str] = Field(None,description="linkedin profile for user discovery")
    twitter: typing.Optional[str] = Field(None,description="twitter profile for user discovery")

    description: typing.Optional[str] = DefaultEmbeddingField(default='', description="A learned description of the user")
    #current_chat_thread: typing.Optional[typing.List[str]] = Field(default_factory=list, description="A thread is a single conversation on any channel - the session contains a list of messages with thread id binding them")
    recent_threads: typing.Optional[typing.List[dict]|dict] = Field(default_factory=list, description="A thread is a single conversation on any channel - the session contains a list of messages with thread id binding them")
    last_ai_response: typing.Optional[str] = Field(None, description="We store this context for managing conversations and state")
    interesting_entity_keys: typing.Optional[dict] = Field(default_factory=dict, description="For the agents convenience a short term mapping of interesting keys with optional descriptions based on user activity")
    token: typing.Optional[str] = Field(None, description="A token for user authentication to Percolate")
    token_expiry: typing.Optional[datetime.datetime] = Field(None, description="Token expiry datetime for user authentication")
    session_id: typing.Optional[str] = Field(None, description="Last session ID for the user")
    last_session_at: typing.Optional[datetime.datetime] = Field(None, description="Last session activity timestamp")
    roles: typing.Optional[typing.List[str]] = Field(default_factory=list, description="A list of roles the user is a member of - DEPRECATED! use groups instead")
    
    # Security fields for row-level security
    role_level: typing.Optional[int] = Field(AccessLevel.PUBLIC, description="User's role level for security (0=God, 1=Admin, 5=Internal, 10=Partner, 100=Public)")
    groups: typing.Optional[typing.List[str]] = Field(default_factory=list, description="List of groups the user belongs to")
    
    graph_paths: typing.Optional[typing.List[str]] = Field(None, description="Track all paths extracted by an agent as used to build the KG")
    metadata: typing.Optional[dict] = Field(default_factory=dict, description="Arbitrary user metadata")
    email_subscription_active: typing.Optional[bool] = Field(False, description="Users can opt in and out of emails")
    userid: typing.Optional[uuid.UUID| str] = Field(None, description="Allow setting user id by default")
      
    @model_validator(mode='before')
    @classmethod
    def _ids(cls,values):
        
        """special case for user owning their own record"""
        values['userid'] = values.get('userid') or values.get('id')
        
        return values
    
    @staticmethod
    def id_from_email(email):
        return make_uuid(email)
    
    @staticmethod
    def get_userid_from_username(name:str):
        """map the user id from the email address or slack name"""
        from percolate.services import PostgresService
        pg = PostgresService()
        Q = f"""
                SELECT * FROM p8."User" where LOWER(email) = LOWER(%s) or LOWER(slack_id) = LOWER(%s) limit 1
        """
        
        data = pg.execute(Q, data=(name, name))
        if data:
            return data[0]['id']
    
    def as_memory(self,**kwargs):
        """the user memory structure"""

        return {
            'Info': "You can use the users context - observe the current chat thread which may be empty when deciding if the user is referring to something they discussed recently or a new context."
                    "When you do use this context do not explain that to the user as it would be jarring for them. Freely use this context if its relevant or necessary to understand the user context."
                    "The last AI Response from the previous interaction is added for extra context and can be used if the user asks a follow up question in reference to previous response only. But dont ask them for confirmation."
                    "If entity keys are provided you can use the get-entities lookup function to load and inspect them."
                    "some tools may accept a user id and you can use the user id here for those tool calls e.g. to do user specific searches",
            'recent_threads': self.recent_threads,
            'last_ai_response': self.last_ai_response,
            'interesting_entity_keys': self.interesting_entity_keys,
            'users_name': self.name,
            'about user' : self.description,
            'user_id': self.id
        }
        
class UserFact(AbstractModel):
    """
    important information or trivial about a user typically submitted by a user over time.
    we can search specifically for user submitted data using vector search or entity lookup for labeled data.
    """
    id: uuid.UUID| str  
    name: typing.Optional[str] = Field(None,description="A unique entity name e.g. user_id-label")
    label: typing.Optional[str] = Field(None,description="The friendly label for the piece of information")
    description: typing.Optional[str] = DefaultEmbeddingField(default='', description="A description from the user e.g. my nick name is, my favourite color is, i have traveled to, i like foods like...")
    invalidated: typing.Optional[bool] = Field(False, description="We can store old information but invalidate it - if we retrieve such information we may discard it as relevant")
    graph_paths: typing.Optional[typing.List[str]] = Field(None, description="Track all paths extracted by an agent as used to build the KG")
    userid: str
    
    @classmethod
    def save_user_fact(cls, label:str, description:str, user_id: str, graph_paths:typing.List[str]=None):
        """save the user fact with a unique label for the information
        
        Args:
            description: details about the user fact
            unique_label: a user level unique label about the fact - should not collide with other user facts (best effort)
            user_id: the user id supplied in context- cif not known do not try to use this function
            graph_paths: graph paths are tags of the form A/B where A is more specific than B e.g. LLMs/AI
        """
        name = f"{user_id}.{label}"
        id = make_uuid(name)
        u = UserFact(id=id,name=name, label=label,description=description, graph_paths=graph_paths,userid=user_id)
        return p8.repository(u).update_records(u)
    
    @classmethod
    def search_user_facts(cls, query:str, user_id:str):
        """
        Search semantically filtered by a given user id
        
        Args:
            query: a detailed question to search for
            user_id: str the provided user id if known - if not known, do not try to use this function
        """
        
        """for now just search all but filter by user id assumed"""
        return p8.repository(UserFact).search(query,user_id=user_id)
    
    @classmethod
    def get_user_entity(cls, label:str,user_id:str):
        """looks up a user entity using the label qualified with the user id"""
        name = f"{user_id}.{label}"
        return p8.repository(UserFact).get_entities(name)
        
        
class Resources(AbstractModel):
    """Generic parsed content from files, sites etc. added to the system.
    If someone asks about resources, content, files, general information etc. it may be worth doing a search on the Resources.
    If a user expresses interests or preferences or trivia about themselves - you can save it in the background but response naturally in conversation about it.
    If the user asks information about themselves you can also try to search for user facts if you dont have the answer.
    You may want to look at recent "top n" resources for a user or do a semantic search or a combination if the user is interested in deeper analysis.
    
    When responding try to make the format pretty when relevant. For quick answer it does not matter but for larger content using pretty Markdown structures like
    fenced codeblocks, headings, lits, tables, links etc.
    
    
    If you do not find data when you search please call the help function to find other functions to help the user.
    
    the Metadata in the Resource may provide a clickable link to original documents which you can display for the user's convenience.
    
    """
    id: typing.Optional[uuid.UUID| str] = Field("The id is generated as a hash of the required uri and ordinal")  
    name: typing.Optional[str] = Field(None, description="A short content name - non unique - for example a friendly label for a chunked pdf document or web page title")
    category: typing.Optional[str] = Field(None, description="A content category")
    content: str = DefaultEmbeddingField(description="The chunk of content from the source")
    summary: typing.Optional[str] = Field(None,description="An optional summary")
    ordinal: int = Field(0, description="For chunked content we can keep an ordinal")
    uri: str = Field("An external source or content ref e.g. a PDF file on blob storage or public URI")
    metadata: typing.Optional[dict] = {} #for emails it could be sender and receiver info and date
    graph_paths: typing.Optional[typing.List[str]] = Field(None, description="Track all paths extracted by an agent as used to build the KG")
    resource_timestamp: typing.Optional[datetime.datetime] = Field(None,description= 'the database will add a default date but sometimes we want to control when the resource is relevant for - for example a daily log')
    userid:  typing.Optional[uuid.UUID| str] = Field(None, description="The user id is a system field but if we need to control the user context this is how we do it")  
    groupid:  typing.Optional[uuid.UUID| str] = Field(None, description="The group id is a system field but if we need to control the user context this is how we do it - this is for role based access")  
    
    @model_validator(mode='before')
    @classmethod
    def _f(cls, values):
        if not values.get('id'):
            """you need to be careful with the id i.e. ordinals and uris should make the resource unique
            filenames for example should not be used but replaced with e.g. an s3 global uri
            """
            values['id'] = make_uuid({'uri': values['uri'], 'ordinal': values.get('ordinal') or 0})
            
            """default - set it if you care"""
            if not values.get('resource_timestamp'):
                values['resource_timestamp'] = get_iso_timestamp()
                
        return values
    
    @classmethod
    def search_facts_by_users(cls, query:str, user_id:str):
        """
        Search semantically filtered by a given user id
        
        Args:
            query: a detailed question to search for
            user_id: str the provided user id if known - if not known, do not try to use this function
        """
        
        return UserFact.search_user_facts(query, user_id)
    
    @classmethod
    def save_user_fact(cls, description:str, unique_label:str, user_id:str, graph_paths:typing.List[str]):
        """save the user fact with a unique label for the information
        
        Args:
            description: details about the user fact
            unique_label: a user level unique label about the fact - should not collide with other user facts (best effort)
            user_id: the user id supplied in context- if not known do not try to use this function
            graph_paths: graph paths are tags of the form A/B where A is more specific than B e.g. LLMs/AI
        """
        
        return UserFact.save_user_fact(unique_label, description=description,user_id=user_id, graph_paths=graph_paths)

    @classmethod
    def chunked_resource_from_text(cls,
        text: str,
        uri:str,
        chunk_size: int = 1000,
        category: typing.Optional[str] = None,
        name: typing.Optional[str] = None,
        userid: typing.Optional[str] = None
    ) -> typing.List["Resources"]:
        """load text into simple chunked resource"""
        name = name or uri
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

        resources = []
        for i, chunk in enumerate(chunks):
            """TODO: NB!!!!!! HERE WE ARE USING URIS WITHOUT PARAMETERS AS GLOBALLY UNIQUE"""
            id_input = f"{uri.split('?')[0]}-{i}"
            id_hash = make_uuid(id_input)

            resource = cls(
                id=id_hash,
                name=f'{name} ({i})' if i > 0 else name,
                category=category,
                content=chunk,
                ordinal=i,
                uri=uri,
                userid=userid
            )
            resources.append(resource)

        return resources
        
        
    # chunked_resource_old has been removed - use chunked_resource instead
    
    @classmethod
    def chunked_resource(
        cls,
        uri: str,
        parsing_mode: typing.Literal["simple", "extended"] = "simple",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        category: typing.Optional[str] = None,
        name: typing.Optional[str] = None,
        userid: typing.Optional[str] = None,
        metadata: typing.Optional[typing.Dict[str, typing.Any]] = None,
        save_to_db: bool = True
    ) -> typing.List["Resources"]:
        """
        Create chunked resources using the FileSystemService unified approach.
        
        This method delegates to FileSystemService.read_chunks to ensure consistent
        behavior across all file processing workflows.
        
        Args:
            uri: File URI (local file://, S3 s3://, or HTTP/HTTPS URL)
            parsing_mode: "simple" for basic text extraction, "extended" for LLM-enhanced parsing
            chunk_size: Number of characters per chunk
            chunk_overlap: Number of characters to overlap between chunks
            category: Optional category for the resources
            name: Optional name for the resources
            userid: Optional user ID to associate with resources
            metadata: Optional metadata to include with resources
            save_to_db: Whether to save the resources to the database
            
        Returns:
            List of Resources representing the chunks
            
        Raises:
            ValueError: If file type is not supported or transcription is required for audio/video in simple mode
            Exception: If parsing fails
        """
        from percolate.services.FileSystemService import FileSystemService
        
        # Use FileSystemService directly to avoid circular dependencies
        fs = FileSystemService()
        
        try:
            # Use read_chunks with ResourceChunker directly to avoid the circular call
            # We call the ResourceChunker directly since read_chunks would call back to this method
            # Use the new location of ResourceChunker in parsing directory
            chunker = None
            if hasattr(fs, '_get_resource_chunker'):
                try:
                    chunker = fs._get_resource_chunker()
                except:
                    chunker = None
            if not chunker:
                # Fallback: import ResourceChunker directly
                from percolate.utils.parsing.ResourceChunker import create_resource_chunker
                chunker = create_resource_chunker(fs)
            
            resources = chunker.chunk_resource_from_uri(
                uri=uri,
                parsing_mode=parsing_mode,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                user_id=userid,
                metadata=metadata
            )
            
            # Override category and name if provided
            for resource in resources:
                if category:
                    resource.category = category
                if name:
                    # Modify the name to include chunk info
                    chunk_index = resource.metadata.get('chunk_index', 0)
                    if chunk_index > 0:
                        resource.name = f"{name} (chunk {chunk_index + 1})"
                    else:
                        resource.name = name
            
            # Save to database if requested
            if save_to_db:
                import percolate as p8
                p8.repository(cls).update_records(resources)
                from percolate.utils import logger
                userid = resources[0].userid if resources else None
                logger.info(f"Saved {len(resources)} chunked resources to database for URI: {uri} with userid: {userid}")
            
            return resources
            
        except Exception as e:
            from percolate.utils import logger
            logger.error(f"Error creating chunked resources from {uri}: {str(e)}")
            raise
    
    @classmethod
    def get_recent_uploads_by_user(
        cls,
        user_id: str,
        limit: int = 10
    ) -> typing.List[typing.Dict[str, typing.Any]]:
        """
        Get the most recent resource uploads for a specific user.
        
        Args:
            user_id: The user ID to filter resources by
            limit: Maximum number of resources to return (default: 10)
            
        Returns:
            List of resource records as dictionaries, ordered by created_at descending
        """
        from percolate.services import PostgresService
        
        # Create a repository for the Resources model
        pg = PostgresService(model=cls)
        
        # Build and execute the query
        query = f"""
            SELECT * 
            FROM {pg.helper.table_name}
            WHERE userid = %s
            ORDER BY created_at DESC
            LIMIT %s
        """
        
        return pg.execute(query, data=(user_id, limit))

class IntervalResource(AbstractModel):
    """
    Interval Resources can be created from other resources as summaries.
    Graph paths are topic's Specific-Item/General-Topic that can be woven into the graph. When generating interval resources we can extend the knowledge graph.
    
    To create moments look for recent uploads to some limit and then return a collection of resource chunks that describe periods of time where the user was interested
    in one or more things. Each moment can have a title and a unique id such as the hash of the title and date.
    
    Depending on the resources, you should create 1-5 interval/moments per day with a density of about 5-10 resources for interval/moment
    
    """
    end_date: typing.Optional[datetime.datetime] = Field(description="Interval resources are conceptual resources that span time")
    
    @classmethod
    def get_recent_user_uploads(cls, user_id:str, limit: int = 20):
        """
        Loads recent uploads for the user so that we can build a model of their interests
        """
        
        return p8.repository(Resources).execute(f""""
                                    SELECT name, content, created_at, category 
                                    from p8."Resources"
                                    where userid = %s 
                                    order by created_at desc
                                    limit %s     
                                         """,
                                         data=(user_id,limit))
        
    @classmethod
    def save_interval_resources(cls, records: typing.List[dict]):
        """use the model schema for Interval Resources to create a collection for saving
        include graph_paths, content, name, userid
        """ 
        
        try:
            records = p8.repository(IntervalResource).update_records(records)
            return {
                "status": "updated",
                "record_count": len(records)
            }
        except:
            import traceback
            logger.warning(f"{traceback.format_exc()}")
            raise
        
class TaskResources(AbstractModel):
    """(Deprecate)A link between tasks and resources since resources can be shared between tasks"""
    id: typing.Optional[uuid.UUID| str] = Field(None, description="unique id for rel" )  
    resource_id: uuid.UUID| str = Field("The resource id" )  
    session_id:  uuid.UUID| str = Field("The session id is typically a task or research iteration but can be any session id to group resources" )  
    user_metadata: typing.Optional[dict] = Field(default_factory=dict, description="User-specific metadata for this resource")
    relevance_score: typing.Optional[float] = Field(default=None, description="How relevant this resource is to the session")
    @model_validator(mode='before')
    @classmethod
    def _f(cls, values):
        if not values.get('id'):
            """
            """
            values['resource_id'] = str(values['resource_id'])
            values['session_id'] = str(values['session_id'])
            
            values['id'] = make_uuid({'resource_id': values['resource_id'], 'session_id': values.get('session_id') or 0})
            
        return values
    
class SessionResources(AbstractModel):
    """A link between sessions and resources since resources can be shared between sessions"""
    id: typing.Optional[uuid.UUID| str] = Field(None, description="unique id for rel" )  
    resource_id: uuid.UUID| str = Field("The resource id" )  
    session_id:  uuid.UUID| str = Field("The session id is any user intent from a chat/request/trigger" )  
    count: typing.Optional[int] = Field(1, description="In a model where we only store head we can track an estimate for chunk count")
    
    @model_validator(mode='before')
    @classmethod
    def _f(cls, values):
        if not values.get('id'):
            """
            """
            values['resource_id'] = str(values['resource_id'])
            values['session_id'] = str(values['session_id'])
            values['id'] = make_uuid({'resource_id': values['resource_id'], 'session_id': values.get('session_id') or 0})
        return values
class _QuestionSet(BaseModel):
    query:str = Field(description="The question/query to search")
    concept_keys: typing.Optional[typing.List[str]] = Field(default_factory=list, description="A list of codes related to the concept diagram matching questions to the research structure")
    
class ResearchIteration(AbstractModel):
    """
    Your job is to use functions to execute research tasks. You do not search for answers directly, you post a research tasks to generate data.
    A research iteration is a plan to deal with a task.     
    If you are asked for a plan you should first use your json structure to create a plan and give it to the user.

    1. if you are asked to information on general topics, you should execute the post_tasks_research_execute
    2. if you are asked about other research iterations only then can you use the _search_ method which is designed to search research plans as opposed to actually search for general information.
    
    You can generate conceptual diagrams using mermaid diagrams to provide an overview of the research plan.
    When you generate a conceptual plan you should link question sets to plans for example each question should have labels that link to part of the conceptual diagram using the mermaid diagram format to describe your plan.
    """
    id: typing.Optional[uuid.UUID| str] = Field(None, description="unique id for rel" )  
    iteration: int
    content: typing.Optional[str] = DefaultEmbeddingField(None,description="An optional summary of the results discovered")
    conceptual_diagram: typing.Optional[str] = Field(None, description="The mermaid diagram for the plan - typically generated in advanced of doing a search")
    question_set: typing.List[_QuestionSet] = Field(description="a set of questions and their ids from the conceptual diagram")
    task_id: typing.Optional[uuid.UUID| str] = Field(default=None,description="Research are linked to tasks which are at minimum a question" )  
    
    @classmethod
    def get_model_functions(cls):
        """override model functions to provide my available behaviours/tools"""
        return {
            'post_tasks_research_execute': "post the ResearchIteration object to the endpoint to execute a research plan."
        }
        
    """we add some functions for illustration and testing but the database should be used for core functions"""
    # @classmethod
    # def perform_web_search(cls,query:str,limit:int=3)->typing.List[dict]:
    #     """performs a tavily web search - supply a query and receive a set of summaries and urls
        
    #     Args:
    #         query: the long form web query to search using the search api
    #         limit: how many search results to fetch
    #     """
        
    #     from percolate.utils.env import TAVILY_API_KEY
    #     import requests
        
    #     if not TAVILY_API_KEY:
    #         raise Exception(f"The TAVILY_API_KEY key is not set so we have no way to search")
        
    #     headers = { "Authorization": f"Bearer {TAVILY_API_KEY}"  }
        
    #     r  = requests.post("https://api.tavily.com/search", headers=headers, json={'query':query,'limit':limit})
    #     if not r.status_code in [200,201]:
    #         raise Exception(f"Error calling function - {r.content}")
    #     return r.json()
    

    
    @classmethod
    def fetch_web_content(cls,url:str, summarization_context:str=None):
        """provide a uri and fetch the web content - some pages not be accessible and you should typically report the error and try another resource
        
        Args:
            url: the web page to fetch - content will be converted to markdown
            summarization_context: it is recommended to pass a summarization context to reduce propagated tokens - the page will be summarized once in context
        """
        import html2text
        import requests
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
        }
        
        data = requests.get(url, headers=headers).content.decode()
        data = html2text.html2text(data)
        if summarization_context:
            data = p8.summarize(data,summarization_context)
        return data
        
class BlockDocument(AbstractModel):
    """Generic parsed content from files, sites etc. added to the system"""
    id: typing.Optional[uuid.UUID| str] = Field("The id is generated as a hash of the required uri and ordinal")  
    name: typing.Optional[str] = Field(None, description="A short content name - non unique - for example a friendly label for a chunked pdf document or web page title")
    category: typing.Optional[str] = Field(None, description="A content category")
    summary: str = DefaultEmbeddingField(description="The chunk of content from the source")
    #conceptual blocks show document structure; inter block links, citations, entities referenced
    content_blocks:typing.List[dict] = Field(description="The content blocks in json format")
    conceptual_diagram: typing.Optional[str] = Field(None, description="The mermaid diagram for the plan or renewed for the document")
    metadata: typing.Optional[dict] = {} #metadata for a document
    
class PercolateAgent(Resources):
    """The percolate agent is the guy that tells you about Percolate which is a multi-modal database for managing AI in the data tier.
    You can learn about the philosophy of Percolate or ask questions about the docs and codebase.
    You can lookup entities of different types or plan queries and searches.
    You can call any registered apis and functions and learn more about how they can be used.
    Call the search function to get data about Percolate
    Only use 'search' if you are asked questions specifically about Percolate otherwise call for help!
    If you do a search and you do not find any data related to the user question you should ALWAYS ask for help to dig deeper.
    """
    @model_validator(mode='before')
    @classmethod
    def _f(cls, values):
        if not values.get('id'):
            values['id'] = make_uuid({'uri': values['uri'], 'ordinal': values['ordinal']})
        return values


class Settings(AbstractModel):
    """settings are key value pairs for percolate admin"""
    id: typing.Optional[uuid.UUID| str] = Field("The id is generated as a hash of the required key and ordinal. The id must be in a UUID format or can be left blank")  
    key: str = Field(default="The key for the value to store - id is generated form this")
    value: str = Field(description="Value of the setting")
    
    @model_validator(mode='before')
    @classmethod
    def _f(cls, values):
        values['id'] = make_uuid({"key": values['key']})
        return values
    
    @classmethod
    def inserts_from_map(cls, values:dict):
        """given some key value settings, generate an insert batch statement - might be a good idea to wrap these in try blocks """
        if not values:
            return ""

        statements = []
        for key, val in values.items():
            # Convert value to JSON-safe string, then escape for SQL
            escaped_key = json.dumps(key)
            sql_key = f"'{key}'"
            escaped_val = f"'{val}'"

            """we use a database function that hashes the key for the id and then we add the key value pair"""
            stmt = f"""
INSERT INTO p8."Settings" (id, key, value)
SELECT p8.json_to_uuid('{{"key": {escaped_key}}}'::JSONB), {sql_key}, {escaped_val}
ON CONFLICT (id)
DO UPDATE SET value = EXCLUDED.value;
""".strip()
            statements.append(stmt)

        return "\n".join(statements)
  
# Scheduled tasks model
class Schedule(AbstractModel):
    """Defines a scheduled task.
    Tasks are scheduled by the system or by users and can be a system prompt for an agent or a function call
    """
    id: typing.Optional[uuid.UUID| str] = Field(None, description="Unique schedule id")
    userid: typing.Optional[uuid.UUID| str] = Field(None, description="User id associated with schedule")
    name: str = Field(..., description="Task to execute")
    spec: dict = Field(..., description="the task spec - The task spec can be any json but we have an internal protocol. LLM instructions are valid should use a system_prompt attribute")
    schedule: str = Field(..., description="Cron schedule string, e.g. '0 0 * * *'")
    disabled_at: typing.Optional[datetime.datetime] = Field(None, description="Time when schedule was disabled")

    @model_validator(mode='before')
    @classmethod
    def _set_defaults(cls, values):
        if isinstance(values, dict) and not values.get('id'):
            values['id'] = str(uuid.uuid1())
        return values
 
 ###
 ## TODO: the functions on this object will move to SQL function and API level 
 ###
class Engram(AbstractModel):
    """Your role is to observe user text and then create a user memory.
    A user memory is a link between a `User` node named <user> and a <relationship> and a named concept.
    You should choose from canonical relationship names if you can;
    
    - only use knows relationships for person knows person
    - feel free to skip relationships that are not defined in the list below
    
    Dont assume strong relationships e.g. if someone is 'doing' something that does not mean they are 'skilled' in it.
    You should look for strong evidence to support each relationship or not mention it at all.........................
        
    **Relationship: **
    - `knows`
    - `likes`
    - `likes-not` 
    - `has-allergy` 
    - `has-goal`
    - `has-skill`
    - `working-on`
    - `is-learning`
    - `recommends`
    - `has-value`
    - `fears`
    - `takes-medication-for` 
    - `plans-to-attend` 
    - `advocates-for`
    - `opposes`
    - `teaches`
    - `aspires-to`
    - `reports-to`
    - 'observed'
    """
    
    name: str = Field(None, description="Item name")

    @classmethod
    def _add_memory_from_user_sessions(cls, since_days_ago:int=1, limit:int=200, user_email:str=None):
        """
        A helper method to read the data from the sessions and generate memories
        
        Args:
            since_days_ago: how far back in days to look for updated session records
            limit: primarily not to overload models, limit the records. 
            Users are unlikely to have more than 100 sessions but file uploads count as sessions
            user_email: optional email filter to only process sessions for a specific user
        """
        
        from percolate.utils import get_days_ago_iso_timestamp 
        dt = get_days_ago_iso_timestamp(n=since_days_ago)
        
        """NOTE: we need to harden these types"""
        #in this case this filters things that are not based on user comments
        session_type ='conversation'
        
        # Build the query with optional email filter
        if user_email:
            data = p8.repository(Session).execute(f""" SELECT a.query, a.created_at, a.userid, a.updated_at, email, u.name
                                            FROM p8."Session" a
                                             join p8."User" u on a.userid = u.id 
                                            where a.updated_at > %s 
                                            and session_type = %s
                                            and u.email = %s
                                            order by a.updated_at desc 
                                            limit {limit}""", data=(dt, session_type, user_email))
        else:
            data = p8.repository(Session).execute(f""" SELECT a.query, a.created_at, a.userid, a.updated_at, email, u.name
                                            FROM p8."Session" a
                                             join p8."User" u on a.userid = u.id 
                                            where a.updated_at > %s 
                                            and session_type = %s
                                            order by a.updated_at desc 
                                            limit {limit}""", data=(dt, session_type))
        
        logger.debug(f"Fetched {len(data)} records to convert to memories")
        
        if not data:
            return
        
        return p8.Agent(Engram).run(f"""Please add memories for the users by their name
                                    - the user_name comes from the email in the data 
                                    - check the function signature for parameter structure
                                    ```json
                                    {json.dumps(data,default=str)}
                                    ```
                                    """)
            
    @classmethod
    def add_memory(cls, relationships: typing.List[dict]):
        """
        Add a user memory by linking a user to a concept by a relationships type
        You should determine the user name from the content
        Te relationships are a list of dicts with `user_name`, `relationship` and `target_name`
        For target names try to use simple keywords and phrases without special characters or punctuation
        
        Args:
            relationships: a list of relationships object|dicts with `user_name`, `relationship` and `target_name`
        """
        
        from percolate.services import PostgresService
        
        for r in relationships:
            r['source_label'] = 'User'
            r['source_name'] = r['user_name']
            r['rel_type'] = r['relationship'].replace('-','_')
            
        r = PostgresService().graph.add_relationships(relationships)
                
        return r
    
    @classmethod
    def describe_user(cls, user_name:str):
        """supply a user name and get their memory graph to construct a narrative for that person 
        - feel free to infer some things the person might do or like but be clear if its assumed
        Make the description conversational rather than bullet points.
        Args:
            user_name: the unique user name as given
        """
        from percolate.services import PostgresService
         
        return PostgresService().graph.get_user_concept_links(user_name,as_model=True)
        

class AuditStatus(Enum):
    """Status enum for audit"""
    Fail="Fail"
    Success="Success"
class Audit(AbstractModel):
    """Generic event audit for Percolate"""
    id: typing.Optional[uuid.UUID| str] = Field(None, description="Unique id")
    userid: typing.Optional[uuid.UUID| str] = Field(None, description="Optional user context")
    status: str = Field(..., description="audit status e.g. Fail|Success")
    caller:str = Field(..., description="The calling object e.g. a class or service")
    status_payload: typing.Optional[dict] = Field(default_factory={}, description="Details about the event")
    error_trace : typing.Optional[str] = Field(None, description="If there is an error dump the stack trace")


# """The daily digest agent will evolve with the help of some API utils
# - we want to summarize changes in the user including their readings etc
# - we want to summarize new resources they have uploaded 
# - we want to specifically look at their evolving memory graph
# """
class DigestAgent(AbstractModel):
    """You are the digestion agent for the user - generate daily or weekly digests to email to the user.
    - Users upload resources like images, document, audio etc and we process them every day
    - Users also interact via that sessions
    - As users interact we build a knowledge graph for their memories. 
    Every time period you should look at what is changing and give a user summary. Create sections. 
    Only add the section when there is something relevant and do not comment on the fact that there is nothing to add if there is not.
    1. Provide a brief summary of the user; their goals, fears, thoughts and dreams as though you are taking to them like an oracle or good friend
    2. Do not be to nice to them. Be polite but challenge them to be better and to grow
    3. Call out any interesting information related to their goals
    4. Call out any appointments or reminders that came up that day that they may want to recall
    5. When looking at graph edges, observe any new edges added based on timestamps
    6. Call out an logical or factual inaccuracies or any conflicts or contradictions that they may want to
    7. Suggest any tasks they may want to keep an eye to get closer to their goals and any new tasks they may want to initiate (you may need access to their full task list)
    8. If you have access to their calendar (disabled for now) list any appointments coming up in the following day or two
    
    IMPORTANT: When upload_analytics is provided in the content:
    - Create an "Upload Activity Summary" section showing:
      - Total files uploaded in last 24 hours
      - How many successfully created resources
      - Total data volume uploaded (use the human-readable format)
      - List the specific files uploaded if available
      - Highlight any failed uploads that need attention
    - Connect uploaded resources to the user's goals when relevant
    - If resource categories are available, group uploads by category
    
    IMPORTANT: When recent_resources_summary is provided:
    - Show the total number of resources created
    - List the categories of resources
    - Show a sample of recent resource names (not all of them)
    - Keep the summary concise and focused on insights
    
    ## Using User Engrams
    - refer to the user by name if you can instead of email
    User engrams are added for your context but dont play this back as a summary but use it to understand recent activity.
    When generating content for the user, in the digest only comment on things that have time stamps for the period and not on summary data.
    Engrams can aggregate over all time so we do not want to repeat this information to the user while new sessions and resources are worth mentioning.
    
    # Tone
    - done not placate the user or add generic commentary - state facts but in an interesting way with suggestions on what to work on
    - comment if something is rare or interesting or noteworthy if it really is i.e. sparingly
    - you are the uer's thinking partner and you want them to think smarter over time. you dont need to tell them this.
    - you can be sparing with adjectives which can often be overused and dilute overall messaging.
    
    
    """
    @classmethod
    def get_daily_digest(cls, name:str, user_name:str, user_engram=None, **kwargs):
        """
        The daily digest will be a combination of data feeds which could be pushed into the data tier
        It should be resource uploaded in the last N hours and the users profile
        Returns formatted HTML content for the digest
        
        Args:
            name: The digest type (e.g., 'Daily Digest')
            user_name: The user's email address
            user_engram: The user's engram/memory data from _add_memory_from_user_sessions
        """
        from percolate.utils import get_days_ago_iso_timestamp
        from datetime import datetime
        
        #fetch by the user name which is the email address
        user = p8.repository(User).execute(""" SELECT * FROM p8."User" where email = %s """, data=(user_name,))
        if not user:
            raise Exception(f"Could not find user by name `{user_name}`")
        if isinstance(user,list):
            user = user[0]
        user_id = user['id']
        
        # Get user's knowledge graph summary - top concepts and relationships
        try:
            graph_summary = p8.repository(User).execute("""
                WITH graph_data AS (
                    SELECT * FROM cypher_query(
                        'MATCH (u:User {name: ''' || %s || '''})-[r]->(c:Concept)
                         RETURN c.name as concept, type(r) as relationship, count(*) as strength
                         ORDER BY count(*) DESC
                         LIMIT 10',
                        'concept agtype, relationship agtype, strength agtype'
                    )
                )
                SELECT 
                    concept::text as concept,
                    relationship::text as relationship_type,
                    strength::int as connection_strength
                FROM graph_data
            """, data=(user_name,))
        except:
            # Graph functionality may not be available
            graph_summary = []
        
        # Get session details - simplified without Engram join
        try:
            recent_sessions = p8.repository(Session).execute("""
                SELECT 
                    s.id,
                    s.agent,
                    s.created_at,
                    s.updated_at,
                    EXTRACT(EPOCH FROM (s.updated_at - s.created_at))/60 as duration_minutes
                FROM p8."Session" s
                WHERE s.userid = %s AND s.created_at >= %s
                ORDER BY s.created_at DESC
                LIMIT 10
            """, data=(user_id, get_days_ago_iso_timestamp(n=1)))
        except:
            recent_sessions = []
        
        # Get resource creation patterns
        resource_patterns = p8.repository(Resources).execute("""
            SELECT 
                DATE_TRUNC('hour', created_at) as hour,
                COUNT(*) as resources_created,
                COUNT(DISTINCT category) as unique_categories,
                ARRAY_AGG(DISTINCT category) FILTER (WHERE category IS NOT NULL) as categories
            FROM p8."Resources"
            WHERE userid = %s AND created_at >= %s
            GROUP BY DATE_TRUNC('hour', created_at)
            ORDER BY hour DESC
        """, data=(user_id, get_days_ago_iso_timestamp(n=1)))
        
        # Get resource summary with more details - filter by created_at
        recent_resources_summary = p8.repository(Resources).execute(""" 
            SELECT 
                COUNT(*) as total_resources,
                COUNT(DISTINCT category) as unique_categories,
                ARRAY_AGG(DISTINCT category) FILTER (WHERE category IS NOT NULL) as categories,
                ARRAY_AGG(name ORDER BY created_at DESC) as recent_resource_names,
                SUM(LENGTH(content)) as total_content_size
            FROM (
                SELECT name, category, content, created_at 
                FROM p8."Resources" 
                WHERE userid=%s AND created_at >= %s
                ORDER BY created_at DESC
                LIMIT 20
            ) recent
        """, data=(user_id, get_days_ago_iso_timestamp(n=1)))
        
        # Get detailed recent resources with content preview - filter by created_at
        recent_resources_sample = p8.repository(Resources).execute(""" 
            SELECT 
                id, 
                name, 
                category, 
                uri, 
                created_at,
                LEFT(content, 500) as content_preview,
                LENGTH(content) as content_length
            FROM p8."Resources" 
            WHERE userid=%s AND created_at >= %s 
            ORDER BY created_at DESC 
            LIMIT 10
        """, data=(user_id, get_days_ago_iso_timestamp(n=1)))
        
        # Get recent engrams/messages for context
        try:
            recent_engrams = p8.repository(Engram).execute("""
                SELECT 
                    e.role,
                    LEFT(e.content, 100) as content_preview,
                    e.created_at,
                    s.agent
                FROM p8."Engram" e
                JOIN p8."Session" s ON s.id = e.sessionid
                WHERE s.userid = %s AND e.created_at >= %s
                AND e.role = 'user'
                ORDER BY e.created_at DESC
                LIMIT 5
            """, data=(user_id, get_days_ago_iso_timestamp(n=1)))
        except:
            recent_engrams = []
        
        session_count_value = len(recent_sessions) if recent_sessions else 0
        
        # Generate agentic summary based on user's activity and goals
        agentic_summary = None
        if recent_resources_sample or recent_engrams or graph_summary:
            try:
                # Prepare data for the agent - limit content to avoid token limits
                resources_for_agent = []
                for r in recent_resources_sample[:5]:  # Top 5 resources
                    resources_for_agent.append({
                        'name': r.get('name', ''),
                        'category': r.get('category', ''),
                        'content_preview': r.get('content_preview', '')[:300],  # Limit content
                        'created_at': str(r.get('created_at', ''))
                    })
                
                engrams_for_agent = []
                for e in recent_engrams[:5]:  # Top 5 engrams
                    engrams_for_agent.append({
                        'content': e.get('content_preview', ''),
                        'agent': e.get('agent', ''),
                        'created_at': str(e.get('created_at', ''))
                    })
                
                agent_data = {
                    'user_name': user_name,
                    'user_email': user['email'],
                    'resources': resources_for_agent,
                    'engrams': engrams_for_agent,
                    'graph_concepts': [g.get('concept', '') for g in graph_summary[:5]] if graph_summary else [],
                    'total_resources': recent_resources_summary[0].get('total_resources', 0) if recent_resources_summary else 0,
                    'categories': recent_resources_summary[0].get('categories', []) if recent_resources_summary else [],
                    'user_engram_insights': user_engram  # Include the user's engram/memory data
                }
                
                agentic_response = p8.Agent(DigestAgent).run(f"""
                    Generate a personalized, narrative summary for {user_name}'s daily digest.
                    Focus on their recent activities, goals, and progress based on the following data:
                    
                    ```json
                    {json.dumps(agent_data, default=str)}
                    ```
                    
                    Your response must be a JSON object with the following structure:
                    {{
                        "narrative_summary": "2-3 paragraph narrative that highlights key themes from their recent uploads and conversations, connects their activities to potential goals or interests, incorporates insights from user_engram_insights if available, and provides encouragement about their knowledge building journey. Be specific about content they engaged with, but keep it concise and engaging. Write in a warm, professional tone.",
                        "extracted_tasks": [
                            {{
                                "task": "Clear action item or task mentioned in their content",
                                "source": "Brief note about where this was found (e.g., 'from audio recording X' or 'mentioned in document Y')",
                                "priority": "high|medium|low"
                            }}
                        ]
                    }}
                    
                    IMPORTANT: Carefully analyze all content for any mentions of:
                    - Things they need to do or want to do
                    - Goals they've mentioned
                    - Action items or next steps discussed
                    - Reminders they've set for themselves
                    - Projects they're working on
                    - Deadlines or time-sensitive items
                    
                    If no tasks are found, return an empty array for extracted_tasks.
                    Ensure your response is valid JSON.
                """)
                
                # Parse the JSON response
                try:
                    import json as json_module
                    parsed_response = json_module.loads(agentic_response)
                    agentic_summary = parsed_response.get('narrative_summary', '')
                    extracted_tasks = parsed_response.get('extracted_tasks', [])
                except:
                    # Fallback if response is not JSON
                    agentic_summary = agentic_response
                    extracted_tasks = []
                
                logger.info(f"Generated agentic summary for {user_name} with {len(extracted_tasks)} tasks")
            except Exception as e:
                logger.warning(f"Failed to generate agentic summary: {str(e)}")
                agentic_summary = None
                extracted_tasks = []
        
        return {
            'user_last_session': user['updated_at'],
            'digest_scope': name,
            'user': user,
            'graph_summary': graph_summary,
            'recent_sessions': recent_sessions,
            'resource_patterns': resource_patterns,
            'recent_resources_summary': recent_resources_summary[0] if recent_resources_summary else {},
            'recent_resources_sample': recent_resources_sample,
            'recent_engrams': recent_engrams,
            'session_count': session_count_value,
            'agentic_summary': agentic_summary,
            'extracted_tasks': extracted_tasks if 'extracted_tasks' in locals() else [],
            'user_engram': user_engram  # Include the user engram data passed in
        }
    
    @classmethod
    def format_daily_digest_html(cls, digest_data: dict, upload_analytics: dict = None) -> str:
        """
        Format the digest data into beautiful HTML email content
        """
        from datetime import datetime
        import json
        
        # Debug logging
        logger.info("="*80)
        logger.info("DIGEST DATA DEBUG:")
        logger.info(f"Resources Summary: {json.dumps(digest_data.get('recent_resources_summary', {}), default=str)}")
        logger.info(f"Resources Sample Count: {len(digest_data.get('recent_resources_sample', []))}")
        logger.info(f"Sessions Count: {digest_data.get('session_count', 0)}")
        logger.info(f"Upload Analytics: {json.dumps(upload_analytics, default=str) if upload_analytics else 'None'}")
        logger.info("="*80)
        
        # Extract data
        user = digest_data.get('user', {})
        user_email = user.get('email', 'User')
        user_name = user.get('name', user_email)  # Use actual name if available
        # Extract first name for greeting
        first_name = user_name.split()[0] if user_name and ' ' in user_name else user_name
        
        digest_scope = digest_data.get('digest_scope', 'Daily Digest')
        session_count = digest_data.get('session_count', 0)
        resources_summary = digest_data.get('recent_resources_summary', {})
        resources_sample = digest_data.get('recent_resources_sample', [])
        graph_summary = digest_data.get('graph_summary', [])
        recent_sessions = digest_data.get('recent_sessions', [])
        resource_patterns = digest_data.get('resource_patterns', [])
        recent_engrams = digest_data.get('recent_engrams', [])
        
        # Start building HTML
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{digest_scope}</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
            margin: 0;
            padding: 0;
        }}
        .container {{
            max-width: 600px;
            margin: 0 auto;
            background-color: #ffffff;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px 30px;
            text-align: center;
        }}
        .header h1 {{
            margin: 0;
            font-size: 28px;
            font-weight: 600;
        }}
        .header p {{
            margin: 10px 0 0 0;
            opacity: 0.9;
            font-size: 16px;
        }}
        .content {{
            padding: 30px;
        }}
        .section {{
            margin-bottom: 30px;
        }}
        .section-title {{
            font-size: 20px;
            font-weight: 600;
            color: #333;
            margin-bottom: 15px;
            border-bottom: 2px solid #667eea;
            padding-bottom: 8px;
        }}
        .metric-grid {{
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 15px;
            margin-bottom: 30px;
        }}
        .metric-card {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            border: 1px solid #e9ecef;
        }}
        .metric-value {{
            font-size: 36px;
            font-weight: 700;
            color: #667eea;
            margin: 0;
        }}
        .metric-label {{
            font-size: 14px;
            color: #6c757d;
            margin: 5px 0 0 0;
        }}
        .table {{
            width: 100%;
            border-collapse: collapse;
            margin-top: 15px;
        }}
        .table th {{
            background-color: #f8f9fa;
            padding: 12px;
            text-align: left;
            font-weight: 600;
            border-bottom: 2px solid #dee2e6;
            color: #495057;
        }}
        .table td {{
            padding: 12px;
            border-bottom: 1px solid #dee2e6;
        }}
        .table tr:hover {{
            background-color: #f8f9fa;
        }}
        .status-badge {{
            display: inline-block;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: 500;
        }}
        .status-completed {{
            background-color: #d4edda;
            color: #155724;
        }}
        .status-failed {{
            background-color: #f8d7da;
            color: #721c24;
        }}
        .status-pending {{
            background-color: #fff3cd;
            color: #856404;
        }}
        .category-tag {{
            display: inline-block;
            background-color: #e9ecef;
            color: #495057;
            padding: 3px 10px;
            border-radius: 15px;
            font-size: 12px;
            margin-right: 5px;
        }}
        .footer {{
            background-color: #f8f9fa;
            padding: 20px 30px;
            text-align: center;
            color: #6c757d;
            font-size: 14px;
        }}
        .no-data {{
            text-align: center;
            color: #6c757d;
            padding: 20px;
            font-style: italic;
        }}
        @media (max-width: 600px) {{
            .metric-grid {{
                grid-template-columns: 1fr;
            }}
            .container {{
                box-shadow: none;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Your {digest_scope}</h1>
            <p>{datetime.now().strftime('%B %d, %Y')}</p>
        </div>
        
        <div class="content">
"""
        
        # Prepare data for summary
        total_resources = resources_summary.get('total_resources', 0)
        total_uploads = upload_analytics.get('total_uploads_24h', 0) if upload_analytics else 0
        completed_uploads = upload_analytics.get('completed_uploads_24h', 0) if upload_analytics else 0
        total_size = upload_analytics.get('total_size_human_readable', '0 B') if upload_analytics else '0 B'
        categories = resources_summary.get('categories', [])
        
        # Opening greeting
        current_hour = datetime.now().hour
        if current_hour < 12:
            greeting = "Good morning"
        elif current_hour < 17:
            greeting = "Good afternoon"
        else:
            greeting = "Good evening"
        
        html += f"""
            <div class="section">
                <p style="font-size: 18px; line-height: 1.8; margin-bottom: 30px;">
                    {greeting}, <strong>{first_name}</strong>.
                </p>
        """
        
        # Check if we have an agentic summary to use
        agentic_summary = digest_data.get('agentic_summary')
        
        if agentic_summary:
            # Use the AI-generated personalized summary
            # Format the agentic summary for HTML
            formatted_summary = agentic_summary.replace('\n\n', '</p><p style="margin-bottom: 15px;">')
            html += f"""
                <div style="background: #f0f4ff; padding: 25px; border-radius: 10px; margin-bottom: 30px; border-left: 4px solid #667eea;">
                    <h3 style="font-size: 20px; color: #333; margin-bottom: 15px; font-weight: 600;">Your Personalized Insights</h3>
                    <div style="font-size: 16px; line-height: 1.8; color: #444;">
                        <p style="margin-bottom: 15px;">{formatted_summary}</p>
                    </div>
                </div>
            """
        
        # Three-paragraph summary based on actual content (fallback if no agentic summary)
        if not agentic_summary and (total_resources > 0 or total_uploads > 0):
            # Analyze content for summary
            audio_transcripts = []
            documents_processed = []
            key_themes = []
            
            # Extract meaningful content from resources
            for resource in resources_sample:
                category = resource.get('category', '')
                content = resource.get('content_preview', '')
                name = resource.get('name', '')
                
                if category == 'audio_transcription' and content:
                    audio_transcripts.append({
                        'name': name,
                        'content': content.strip()
                    })
                elif category == 'pdf_chunk' and content:
                    doc_name = name.split('(')[0].strip() if '(' in name else name
                    if doc_name not in [d['name'] for d in documents_processed]:
                        documents_processed.append({
                            'name': doc_name,
                            'content': content.strip()[:200]
                        })
            
            # Paragraph 1: Overall activity summary
            html += f"""
                <p style="font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
                    Over the past 24 hours, you've been actively building your knowledge base with 
                    <strong>{total_uploads} uploads</strong> totaling <strong>{total_size}</strong> of data. 
                    These materials were successfully processed into <strong>{total_resources} searchable resources</strong>
                    {f' across {len(categories)} categories: {", ".join(categories)}' if categories else ''}.
                    This consistent activity demonstrates your commitment to organizing and preserving important information.
                </p>
            """
            
            # Paragraph 2: Content insights
            if audio_transcripts:
                # Summarize audio content themes
                audio_topics = []
                for transcript in audio_transcripts[:3]:
                    content_snippet = transcript['content'][:200]
                    if 'BrainBridge' in content_snippet:
                        audio_topics.append("BrainBridge device capabilities")
                    if 'memory' in content_snippet.lower():
                        audio_topics.append("memory module functionality")
                    if 'email digest' in content_snippet.lower():
                        audio_topics.append("digest system development")
                    if 'backend' in content_snippet.lower() or 'upload' in content_snippet.lower():
                        audio_topics.append("technical infrastructure")
                
                topics_text = " and ".join(set(audio_topics[:3])) if audio_topics else "various technical topics"
                
                html += f"""
                <p style="font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
                    Your audio recordings reveal focused work on {topics_text}. 
                    The transcribed conversations show thoughtful consideration of system architecture and user experience, 
                    particularly around how to track and present information effectively. 
                    {'You also processed several PDF documents, creating a comprehensive searchable archive.' if documents_processed else ''}
                </p>
                """
            elif documents_processed:
                html += f"""
                <p style="font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
                    Your document processing focused on {len(documents_processed)} key files, including 
                    {', '.join([d['name'] for d in documents_processed[:2]])}. 
                    Each document was intelligently chunked for optimal searchability, ensuring you can quickly 
                    locate specific information when needed.
                </p>
                """
            else:
                html += f"""
                <p style="font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
                    The processed materials span {', '.join(categories[:3]) if categories else 'multiple categories'}, 
                    creating a diverse knowledge repository. This variety ensures comprehensive coverage 
                    of your areas of interest and work.
                </p>
                """
            
            # Paragraph 3: Looking forward
            html += f"""
                <p style="font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
                    {'Your ' + str(session_count) + ' active sessions show sustained engagement with the platform. ' if session_count > 0 else ''}
                    As your knowledge base grows, these resources become increasingly valuable for future reference 
                    and insight generation. The system continues to learn from your usage patterns to provide 
                    better organization and retrieval capabilities.
                </p>
            """
        else:
            # Quiet day summary
            html += f"""
                <p style="font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
                    It's been a quiet day in your Dreaming Bridge workspace. Sometimes the most productive 
                    days are those spent reflecting on previously gathered knowledge rather than adding new content.
                </p>
                
                <p style="font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
                    Your knowledge base remains ready whenever you need it, with all your previous uploads 
                    organized and searchable. The system continues to maintain and optimize your resources 
                    in the background.
                </p>
                
                <p style="font-size: 16px; line-height: 1.8; margin-bottom: 20px;">
                    When you're ready to add new content, simply upload your files and they'll be processed 
                    and integrated into your growing repository of knowledge.
                </p>
            """
        
        html += """
            </div>
            
            <!-- Invitation to Chat Section -->
            <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 30px; border-radius: 12px; margin: 30px 0; text-align: center; box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);">
                <h2 style="color: white; font-size: 28px; margin-bottom: 15px; font-weight: 600;">
                     Chat with Your Knowledge Base! 
                </h2>
                <p style="color: rgba(255, 255, 255, 0.9); font-size: 18px; line-height: 1.6; margin-bottom: 20px; max-width: 600px; margin-left: auto; margin-right: auto;">
                    Your entire knowledge repository is now conversational. Ask questions, explore insights, and discover connections across all your uploaded content.
                </p>
                <a href="https://vault.percolationlabs.ai" style="display: inline-block; background: white; color: #667eea; padding: 15px 40px; border-radius: 30px; text-decoration: none; font-weight: 600; font-size: 18px; box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1); transition: all 0.3s ease;">
                     Start Chatting Now
                </a>
                <p style="color: rgba(255, 255, 255, 0.8); font-size: 14px; margin-top: 15px; margin-bottom: 0;">
                    Access your personalized AI assistant at <strong>vault.percolationlabs.ai</strong>
                </p>
            </div>
            
            <hr style="border: none; border-top: 1px solid #e9ecef; margin: 40px 0;">
        """
        
        # Add extracted tasks section if available
        extracted_tasks = digest_data.get('extracted_tasks', [])
        if extracted_tasks:
            html += """
            <div style="background: #fff3cd; border: 1px solid #ffeaa7; padding: 25px; border-radius: 10px; margin-bottom: 30px;">
                <h2 style="color: #856404; font-size: 22px; margin-bottom: 20px; display: flex; align-items: center;">
                     Tasks & Action Items Identified
                </h2>
            """
            
            # Group tasks by priority
            high_priority = [t for t in extracted_tasks if t.get('priority') == 'high']
            medium_priority = [t for t in extracted_tasks if t.get('priority') == 'medium']
            low_priority = [t for t in extracted_tasks if t.get('priority') == 'low']
            
            # Display high priority tasks
            if high_priority:
                html += """
                <div style="margin-bottom: 20px;">
                    <h3 style="color: #dc3545; font-size: 16px; margin-bottom: 10px;"> High Priority</h3>
                """
                for task in high_priority:
                    html += f"""
                    <div style="background: white; padding: 12px; border-radius: 6px; margin-bottom: 8px; border-left: 4px solid #dc3545;">
                        <p style="margin: 0; color: #333; font-weight: 500;">{task.get('task', '')}</p>
                        <p style="margin: 4px 0 0 0; font-size: 12px; color: #6c757d; font-style: italic;">
                            Source: {task.get('source', 'Unknown')}
                        </p>
                    </div>
                    """
                html += "</div>"
            
            # Display medium priority tasks
            if medium_priority:
                html += """
                <div style="margin-bottom: 20px;">
                    <h3 style="color: #fd7e14; font-size: 16px; margin-bottom: 10px;"> Medium Priority</h3>
                """
                for task in medium_priority:
                    html += f"""
                    <div style="background: white; padding: 12px; border-radius: 6px; margin-bottom: 8px; border-left: 4px solid #fd7e14;">
                        <p style="margin: 0; color: #333; font-weight: 500;">{task.get('task', '')}</p>
                        <p style="margin: 4px 0 0 0; font-size: 12px; color: #6c757d; font-style: italic;">
                            Source: {task.get('source', 'Unknown')}
                        </p>
                    </div>
                    """
                html += "</div>"
            
            # Display low priority tasks
            if low_priority:
                html += """
                <div style="margin-bottom: 0;">
                    <h3 style="color: #28a745; font-size: 16px; margin-bottom: 10px;"> Low Priority</h3>
                """
                for task in low_priority:
                    html += f"""
                    <div style="background: white; padding: 12px; border-radius: 6px; margin-bottom: 8px; border-left: 4px solid #28a745;">
                        <p style="margin: 0; color: #333; font-weight: 500;">{task.get('task', '')}</p>
                        <p style="margin: 4px 0 0 0; font-size: 12px; color: #6c757d; font-style: italic;">
                            Source: {task.get('source', 'Unknown')}
                        </p>
                    </div>
                    """
                html += "</div>"
            
            html += """
            </div>
            """
        
        # Detail sections (moved down from top)
        if total_uploads > 0 or total_resources > 0:
                # Analyze content to extract topics and themes
                audio_insights = []
                document_insights = []
                
                for r in resources_sample[:10]:
                    name = r.get('name', '')
                    category = r.get('category', '')
                    content = r.get('content_preview', '')
                    
                    if category == 'audio_transcription' and content:
                        # Extract key phrases from audio transcription
                        clean_content = content.strip()[:300]
                        if clean_content and len(clean_content) > 50:
                            audio_insights.append({
                                'name': name,
                                'content': clean_content
                            })
                    elif category == 'pdf_chunk' and content:
                        # Extract document topic
                        doc_name = name.split('(')[0].strip() if '(' in name else name
                        if doc_name not in [d['name'] for d in document_insights]:
                            document_insights.append({
                                'name': doc_name,
                                'content': content.strip()[:200]
                            })
                
                # Build insights narrative
                if audio_insights or document_insights:
                    html += f"""
                    <div style="background: #e9ecef; padding: 20px; border-radius: 8px; margin-bottom: 20px;">
                        <h3 style="font-size: 18px; color: #333; margin-bottom: 15px;"> Content Insights</h3>
                    """
                    
                    # Audio insights
                    if audio_insights:
                        html += """
                        <div style="margin-bottom: 15px;">
                            <h4 style="font-size: 16px; color: #495057; margin-bottom: 10px;"> From your audio recordings:</h4>
                        """
                        for insight in audio_insights[:2]:
                            # Extract meaningful snippet
                            content_snippet = insight['content'][:150]
                            if len(insight['content']) > 150:
                                # Try to end at a word boundary
                                last_space = content_snippet.rfind(' ')
                                if last_space > 100:
                                    content_snippet = content_snippet[:last_space]
                                content_snippet += "..."
                            
                            html += f"""
                            <div style="margin-bottom: 10px; padding: 10px; background: white; border-radius: 6px;">
                                <p style="margin: 0; font-style: italic; color: #495057; line-height: 1.6;">
                                    "{content_snippet}"
                                </p>
                                <p style="margin: 5px 0 0 0; font-size: 12px; color: #6c757d;">
                                     from {insight['name']}
                                </p>
                            </div>
                            """
                        html += """
                        </div>
                        """
                    
                    # Document insights
                    if document_insights:
                        html += """
                        <div>
                            <h4 style="font-size: 16px; color: #495057; margin-bottom: 10px;"> From your documents:</h4>
                            <p style="margin: 0; line-height: 1.8;">
                        """
                        doc_names = [d['name'] for d in document_insights[:3]]
                        html += f"""
                                You've been working with documents including <strong>{', '.join(doc_names)}</strong>, 
                                building your knowledge base with diverse content.
                            </p>
                        </div>
                        """
                    
                    html += """
                    </div>
                    """
        else:
            # Quiet day narrative
            html += f"""
                <div style="background: #f8f9fa; padding: 20px; border-radius: 8px; margin-bottom: 20px;">
                    <p style="margin-bottom: 0; line-height: 1.8; text-align: center; color: #6c757d;">
                        It's been a quiet day in your Dreaming Bridge workspace. 
                        Ready to start uploading and creating new resources?
                    </p>
                </div>
            """
        
        html += """
            </div>
        """
        
        
        
        # Upload Story Section
        if upload_analytics and upload_analytics.get('recent_upload_details'):
            recent_uploads = upload_analytics.get('recent_upload_details', [])
            
            # Group uploads by type
            processed_uploads = []
            failed_uploads = []
            pending_uploads = []
            
            for upload in recent_uploads[:10]:
                status_summary = upload.get('status_summary', '')
                if 'processed' in status_summary.lower():
                    processed_uploads.append(upload)
                elif 'failed' in status_summary.lower():
                    failed_uploads.append(upload)
                else:
                    pending_uploads.append(upload)
            
            html += """
            <div class="section">
                <h2 class="section-title"> Your Upload Journey</h2>
            """
            
            # Success stories
            if processed_uploads:
                html += """
                <div style="margin-bottom: 25px;">
                    <h3 style="font-size: 16px; color: #155724; margin-bottom: 10px;"> Successfully Processed</h3>
                """
                
                # For audio files, we'll look up their content in resources
                audio_contents = {}
                if resources_sample:
                    for resource in resources_sample:
                        if resource.get('category') == 'audio_transcription':
                            # Extract base filename from resource name
                            res_name = resource.get('name', '')
                            content = resource.get('content_preview', '')
                            if res_name and content:
                                audio_contents[res_name] = content
                
                for upload in processed_uploads[:3]:
                    filename = upload.get('filename', 'Unknown')
                    resource_count = upload.get('resource_count', 0)
                    categories = upload.get('categories', [])
                    size = upload.get('total_size', 0)
                    size_mb = size / (1024 * 1024) if size else 0
                    
                    # Build narrative based on file type
                    if filename.endswith('.wav') and filename in audio_contents:
                        # Audio file with transcription
                        transcript = audio_contents[filename][:200].strip()
                        if transcript:
                            html += f"""
                            <div style="margin-bottom: 20px; padding-left: 20px; border-left: 3px solid #28a745;">
                                <p style="margin-bottom: 8px;">
                                    <strong>{filename}</strong> ({size_mb:.1f} MB) - Audio transcription captured
                                </p>
                                <p style="margin: 0; padding: 10px; background: #f8f9fa; border-radius: 6px; font-style: italic; color: #495057;">
                                    "{transcript}..."
                                </p>
                            </div>
                            """
                        else:
                            html += f"""
                            <p style="margin-bottom: 10px; padding-left: 20px; border-left: 3px solid #28a745;">
                                <strong>{filename}</strong> ({size_mb:.1f} MB) - Audio file successfully transcribed
                            </p>
                            """
                    elif filename.endswith('.pdf'):
                        # PDF file
                        chunk_info = f"split into {resource_count} searchable chunks" if resource_count > 1 else "processed"
                        html += f"""
                        <p style="margin-bottom: 10px; padding-left: 20px; border-left: 3px solid #28a745;">
                            <strong>{filename}</strong> ({size_mb:.1f} MB) - PDF document {chunk_info}
                        </p>
                        """
                    else:
                        # Other files
                        category_text = ""
                        if categories and categories[0]:
                            category_text = f" as <em>{categories[0][0]}</em>"
                        
                        html += f"""
                        <p style="margin-bottom: 10px; padding-left: 20px; border-left: 3px solid #28a745;">
                            <strong>{filename}</strong> ({size_mb:.1f} MB) was successfully processed{category_text}
                        </p>
                        """
                
                html += """
                </div>
                """
            
            # Failed uploads
            if failed_uploads:
                html += """
                <div style="margin-bottom: 25px;">
                    <h3 style="font-size: 16px; color: #721c24; margin-bottom: 10px;"> Need Attention</h3>
                """
                
                for upload in failed_uploads[:3]:
                    filename = upload.get('filename', 'Unknown')
                    html += f"""
                    <p style="margin-bottom: 10px; padding-left: 20px; border-left: 3px solid #dc3545;">
                        <strong>{filename}</strong> failed to upload. You may want to try uploading this file again.
                    </p>
                    """
                
                html += """
                </div>
                """
            
            # Summary
            if len(recent_uploads) > 5:
                html += f"""
                <p style="color: #6c757d; font-style: italic; font-size: 14px;">
                    ...and {len(recent_uploads) - 5} more uploads in the last 24 hours.
                </p>
                """
            
            html += """
            </div>
            """
        
        # Resources Created Section
        if resources_sample:
            html += """
            <div class="section">
                <h2 class="section-title"> Your Knowledge Base Growth</h2>
            """
            
            # Group resources by type
            resource_groups = {}
            for resource in resources_sample:
                category = resource.get('category', 'Other')
                if category not in resource_groups:
                    resource_groups[category] = []
                resource_groups[category].append(resource)
            
            # Tell the story of each category
            for category, resources in resource_groups.items():
                if category and resources:
                    html += f"""
                    <div style="margin-bottom: 20px;">
                        <h3 style="font-size: 16px; color: #495057; margin-bottom: 10px;">
                            {category.replace('_', ' ').title()} Resources
                        </h3>
                    """
                    
                    for resource in resources[:2]:  # Show top 2 per category
                        name = resource.get('name', 'Unnamed')
                        content_preview = resource.get('content_preview', '')
                        
                        # Clean up name
                        if '(' in name and ')' in name:  # Remove chunk indicators
                            base_name = name.split('(')[0].strip()
                        else:
                            base_name = name
                        
                        # Clean up preview
                        if content_preview:
                            preview = content_preview.strip()[:150]
                            if len(content_preview) > 150:
                                preview += "..."
                        else:
                            preview = ""
                        
                        html += f"""
                        <div style="margin-bottom: 15px; padding: 15px; background: #f8f9fa; border-radius: 8px;">
                            <p style="margin: 0 0 8px 0; font-weight: 600;">{base_name}</p>
                            {f'<p style="margin: 0; color: #6c757d; font-size: 14px; line-height: 1.6;">{preview}</p>' if preview else ''}
                        </div>
                        """
                    
                    html += """
                    </div>
                    """
            
            # Activity timeline
            if resource_patterns and any(p.get('resources_created', 0) > 0 for p in resource_patterns):
                html += """
                <div style="margin-top: 25px; padding: 20px; background: #e9ecef; border-radius: 8px;">
                    <h3 style="font-size: 16px; color: #333; margin-bottom: 15px;"> Your Activity Timeline</h3>
                    <p style="margin-bottom: 15px; color: #495057;">Here's when you were most active creating resources:</p>
                """
                
                active_hours = []
                for pattern in resource_patterns[:8]:
                    count = pattern.get('resources_created', 0)
                    if count > 0:
                        hour = pattern.get('hour', '')
                        if hour:
                            try:
                                if isinstance(hour, str):
                                    hour_dt = datetime.fromisoformat(hour.replace('Z', '+00:00'))
                                else:
                                    hour_dt = hour
                                hour_str = hour_dt.strftime('%I %p').lstrip('0')
                                active_hours.append(f"<strong>{hour_str}</strong> ({count} items)")
                            except:
                                pass
                
                if active_hours:
                    html += f"""
                    <p style="margin: 0; line-height: 1.8;">
                        Peak activity at: {', '.join(active_hours[:3])}
                        {' and more' if len(active_hours) > 3 else ''}
                    </p>
                    """
                
                html += """
                </div>
                """
            
            html += """
            </div>
            """
        
        # Closing thoughts
        html += """
            <div class="section" style="text-align: center; margin-top: 40px;">
                <p style="color: #6c757d; font-style: italic;">
                    Keep building your knowledge base. Every upload enriches your digital workspace.
                </p>
            </div>
        """
        
        html += """
        </div>
        
        <div class="footer">
            <p>Your {digest_scope} from Dreaming Bridge</p>
            <p style="margin-top: 10px; font-size: 12px; color: #999;">
                Sent with  to help you stay informed about your knowledge workspace
            </p>
        </div>
    </div>
</body>
</html>
""".format(digest_scope=digest_scope)
        
        return html