"""
keeping a log of important integration tests 

[1]
-- it should be possible to route from SQL on the running docker instance to the api and back to the database
-- for example our pg client here uses docker internal as the API exposed on our dev machine
-- the service in turn is configured to use the composed 'postgres' service as the host and its default port
-- the api key is read from the generated key in the instance - i have hard coded below for now though

SELECT token, proxy_uri 
 
FROM p8."ApiProxy"
WHERE name = 'percolate'
LIMIT 1;


SELECT *
FROM public.http(
	( 'POST', 
	'http://host.docker.internal:5008/' || 'admin/index/',
	ARRAY[http_header('Authorization', 'Bearer ' || '40a055a2-3309-b164-489d-4ef8aefc4c1a')],
	'application/json',
	json_build_object('entity_full_name', 'p8.Agent')::jsonb
	)::http_request
);
 
 
[2] it should be possible to run all queies generated by the bootsrapper after docker startup
 - there should be a session entry from percolate init after this
 - there should be a P8_API_KEY in the settings
 - if we run the cli with sync we should have langauge models and leys in LangaugeModelApi after (python percolate/cli.py add env --sync)
 - if we run the indexer - we should have indexed content - the index is generated in embeddings via the API - this is an important integration test
 -- python percolate/cli.py index
 -- check embeddings are added and that there is an index audit for PercolateAgent
 
"""