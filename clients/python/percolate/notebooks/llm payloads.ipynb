{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50635797",
   "metadata": {},
   "source": [
    "# Working with Message Stack payloads \n",
    "- there are currently three support schemes; openai, google, anthropic\n",
    "- these have slightyl different payload structure for tool calls and responses in particular\n",
    "- generally, we want to ACK a tool call with an id and follow it with a response in the message stack\n",
    "- anthropic has a tool block and google has a functionResponse while open AI is easier with just the typically message with role and content\n",
    "- we can test read message stacks as instructions from the database in different contexts\n",
    "    - for a user question; trivial \n",
    "    - for a tool request with tool stack\n",
    "    - for agents that provider system prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ffeeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import percolate as p8\n",
    "from percolate.models.p8 import AIResponse\n",
    "from percolate.services import PostgresService\n",
    "from percolate.services.llm.LanguageModel import request_anthropic,request_google,request_openai\n",
    "from pydantic import BaseModel, model_validator, Field\n",
    "import typing\n",
    "import uuid\n",
    "\n",
    "pg = PostgresService()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3cf688",
   "metadata": {},
   "source": [
    "### illustrate that in each scheme we read messages that are ready for that scheme and functions too\n",
    "- there is a corresponding database request_x that reads data in the same way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873008f0",
   "metadata": {},
   "source": [
    "## The first test should take a singel turn example id\n",
    "- at this point you have created a request with any scheme using e.g. percolate_with_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8486e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_session_id = '8c51c161-7ac8-db55-68ec-7255ea4983e0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78a609ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "goo_mm =  [d for d in pg.execute(f\"\"\" select * from p8.get_google_messages('{test_session_id}') \"\"\")[0]['messages']]  \n",
    "fns =  [d for d in pg.execute(f\"\"\" select * from p8.get_tools_by_name(ARRAY['get_pet_findByStatus'],'google') \"\"\")[0]['get_tools_by_name']]  \n",
    "request_google(goo_mm,fns).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1171d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_mm = [d for d in pg.execute(f\"\"\" select * from p8.get_anthropic_messages('{test_session_id}') \"\"\")[0]['messages']]\n",
    "fns =  [d for d in pg.execute(f\"\"\" select * from p8.get_tools_by_name(ARRAY['get_pet_findByStatus'],'anthropic') \"\"\")[0]['get_tools_by_name']]  \n",
    "request_anthropic(ant_mm,fns).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f5b3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = [d for d in pg.execute(f\"\"\" select * from p8.get_canonical_messages('{test_session_id}') \"\"\")[0]['messages']]\n",
    "fns =  [d for d in pg.execute(f\"\"\" select * from p8.get_tools_by_name(ARRAY['get_pet_findByStatus']) \"\"\")[0]['get_tools_by_name']]  \n",
    "request_openai(mm,fns).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e596c",
   "metadata": {},
   "source": [
    "## Longer turn tests\n",
    "- make sure function calls and responses are paired properly \n",
    "- test injecting in new quuestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9797ca",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1727c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e917237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import percolate as p8\n",
    "from percolate.models.p8 import AIResponse\n",
    "from percolate.services import PostgresService\n",
    "from percolate.services.llm.LanguageModel import request_anthropic,request_google,request_openai, LanguageModel,CallingContext\n",
    "from pydantic import BaseModel, model_validator, Field\n",
    "import typing\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "addcbaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create a simple printer that we can pass down\"\"\"\n",
    "def printer(text):\n",
    "    \"\"\"streaming output\"\"\"\n",
    "    print(text, end=\"\", flush=True)  \n",
    "context = CallingContext(streaming_callback=printer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f94e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fns =[{\n",
    "  \"name\": \"get_weather\",\n",
    "  \"description\": \"Get the weather forecast for a specific city and date\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"city\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The city for which to get the weather forecast\"\n",
    "      },\n",
    "      \"date\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The date for the weather forecast (YYYY-MM-DD)\"\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\"city\", \"date\"]\n",
    "  }\n",
    "}]\n",
    "\n",
    "#this maps to tools by just wrapping\n",
    "tools = [{'type': 'function', 'function': f} for f in fns]\n",
    "\n",
    "models = ['gpt-4o-mini', 'deepseek-chat', 'claude-3-5-sonnet-20241022', 'gemini-1.5-flash']\n",
    "#we can test each of the models ^ \n",
    "#we keep one in eacn scheme - open ai, google and ahtnropic but we also use a second non open ai that uses the same schema to test consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e81a5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-4o-mini', 'deepseek-chat', 'claude-3-5-sonnet-20241022', 'gemini-1.5-flash']\n",
    "#we can test each of the models ^ \n",
    "#we keep one in eacn scheme - open ai, google and ahtnropic but we also use a second non open ai that uses the same schema to test consistency\n",
    "\n",
    "model = LanguageModel(models[0])\n",
    "#1. no stream no tool\n",
    "#model.ask(\"What is the capital of ireland\", functions=fns)\n",
    "# 2 stream no tool - streaming just add a context\n",
    "model.ask(\"What is the capital of ireland\", functions=fns, context=context, debug_response=False)\n",
    "# 3 no stream and tool\n",
    "#model.ask(\"What is the weather in paris tomorrow\", functions=fns)\n",
    "# 4 stream and tool\n",
    "#model.ask(\"What is the weather in paris tomorrow\", functions=fns,  context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686b9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6633672",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(models[1])\n",
    "#1. no stream no tool\n",
    "#model.ask(\"What is the capital of ireland\", functions=fns)\n",
    "# 2 stream no tool - streaming just add a context\n",
    "#model.ask(\"What is the capital of ireland form your world knowledge\", functions=fns, context=context, debug_response=False)\n",
    "# 3 no stream and tool\n",
    "#model.ask(\"What is the weather in paris tomorrow\", functions=fns)\n",
    "# 4 stream and tool\n",
    "#model.ask(\"What is the weather in paris tomorrow\", functions=fns,  context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812f63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(models[2])\n",
    "#1. no stream no tool\n",
    "#model.ask(\"What is the capital of ireland\", functions=fns)\n",
    "# 2 stream no tool - streaming just add a context\n",
    "#model.ask(\"What is the capital of ireland from your world knolwedge\", functions=fns, context=context, debug_response=False)\n",
    "# 3 no stream and tool\n",
    "#model.ask(\"What is the weather in paris tomorrow\", functions=fns)\n",
    "# 4 stream and tool\n",
    "model.ask(\"What is the weather in paris tomorrow\", functions=fns,  context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60f307ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(models[3])\n",
    "#1. no stream no tool\n",
    "#model.ask(\"What is the capital of ireland based on world knowledge\", functions=fns)\n",
    "# 2 stream no tool - streaming just add a context\n",
    "#model.ask(\"What is the capital of ireland from your world knolwedge\", functions=fns, context=context, debug_response=False)\n",
    "# 3 no stream and tool\n",
    "#model.ask(\"What is the weather in paris tomorrow if tomorrow is the 4th of feb 2024\", functions=fns)\n",
    "# 4 stream and tool\n",
    "model.ask(\"What is the weather in paris tomorrow if tomorrow is the 4th of feb 2024\", functions=fns,  context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "### streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6fd672a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {os.environ.get('OPENAI_API_KEY')}\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"messages\": [\n",
    "       # {\"role\": \"user\", \"content\": \"What's the weather in Paris tomorrow?\"}\n",
    "         {\"role\": \"user\", \"content\": \"What's the capital of France?\"}\n",
    "    ],\n",
    "    \"stream\": True,\n",
    "    \"tools\": tools,\n",
    "    \"stream_options\": {\"include_usage\": True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "496f7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:streamGenerateContent?alt=sse&key={os.environ.get('GEMINI_API_KEY')}\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "\n",
    "    \"contents\": [\n",
    "        {\"role\": \"user\", \"parts\": {'text': \"what is the weather for dulin on 2025-02-05 based on the get_weather function?\"}}\n",
    "    ],\n",
    "   \n",
    "   \"tools\": [{'function_declarations': fns}]\n",
    "}\n",
    "r = requests.post(url, headers=headers, data=json.dumps(data), stream=True)\n",
    "\n",
    "import json\n",
    "\n",
    "def stream_google_response(r, printer=None):\n",
    " \n",
    "    # Initialize a variable to hold the current parts of the text\n",
    "    current_text_parts = []\n",
    "\n",
    "    # Process the streamed response\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            # Each chunk of data is prefixed with 'data: ', so we strip that part\n",
    "            line = line.decode(\"utf-8\").strip()\n",
    "            if line.startswith(\"data: \"):\n",
    "                # Remove 'data: ' and parse the JSON\n",
    "                json_data = json.loads(line[len(\"data: \"):])\n",
    "\n",
    "                # Extract the text parts from the streamed chunk\n",
    "                candidates = json_data.get(\"candidates\", [])\n",
    "                for candidate in candidates:\n",
    "                    parts = candidate.get(\"content\", {}).get(\"parts\", [])\n",
    "                    for part in parts:\n",
    "                        text = part.get(\"text\", \"\")\n",
    "                        # Collect the text content and print it\n",
    "                        current_text_parts.append(text)\n",
    "                        print(text)  # Print each part as it comes in\n",
    "\n",
    "                # If the response indicates completion (e.g., 'finishReason': 'STOP'), exit\n",
    "                finish_reason = candidate.get(\"finishReason\")\n",
    "                if finish_reason == \"STOP\":\n",
    "                    print(\"Streaming finished.\")\n",
    "                    break\n",
    "\n",
    "    # Combine all the collected text parts into the final result\n",
    "    final_output = \" \".join(current_text_parts)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "# Example usage (assuming you have a valid `response` object from requests)\n",
    "response = requests.post(url, headers=headers, json=data, stream=True)\n",
    "result = stream_google_response(response, printer=printer)\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "724df011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def printer(text):\n",
    "    \"\"\"streaming output\"\"\"\n",
    "    print(text, end=\"\", flush=True)  \n",
    "\n",
    "def stream_response(r, printer=None):\n",
    "    collected_data = {\n",
    "        'content': '',\n",
    "        'tool_calls': []\n",
    "    }\n",
    "    observed_tool_call = False\n",
    "    tool_args = {}  # {tool_id: aggregated_args}\n",
    "    \n",
    "    for line in r.iter_lines():\n",
    "        if line:\n",
    "            decoded_line = line.decode(\"utf-8\").replace(\"data: \", \"\").strip() \n",
    "            if decoded_line and decoded_line != \"[DONE]\":\n",
    "                try:\n",
    "                    json_data = json.loads(decoded_line)\n",
    "                    \n",
    "                    if \"choices\" in json_data and json_data[\"choices\"]:\n",
    "                        delta = json_data[\"choices\"][0][\"delta\"]\n",
    "                        \n",
    "                        # Check if there's content and aggregate it\n",
    "                        if \"content\" in delta and delta[\"content\"]:\n",
    "                            collected_data['content'] += delta[\"content\"]\n",
    "                            if printer:\n",
    "                                printer(delta[\"content\"])\n",
    "                        \n",
    "                        # Check if there are tool calls and aggregate the arguments\n",
    "                        if \"tool_calls\" in delta:\n",
    "                            if not observed_tool_call:\n",
    "                                observed_tool_call = True\n",
    "                                if printer:\n",
    "                                    printer(delta[\"tool_calls\"])\n",
    "                            for tool_call in delta[\"tool_calls\"]:\n",
    "                                if \"index\" in tool_call:\n",
    "                                    tool_index = tool_call[\"index\"]\n",
    "                                    # Initialize tool call if not already in the dictionary\n",
    "                                    if tool_index not in tool_args:\n",
    "                                        tool_args[tool_index] = \"\"\n",
    "                                    if \"function\" in tool_call and \"arguments\" in tool_call[\"function\"]:\n",
    "                                        tool_args[tool_index] += tool_call[\"function\"][\"arguments\"]\n",
    "                \n",
    "                except json.JSONDecodeError:\n",
    "                    pass  # Handle incomplete JSON chunks\n",
    "    \n",
    "    # Once the stream finishes, populate the tool_calls list with aggregated arguments\n",
    "    for tool_index, args in tool_args.items():\n",
    "        collected_data['tool_calls'].append({\n",
    "            'index': tool_index,\n",
    "            'arguments': args\n",
    "        })\n",
    "    \n",
    "    json_data.update(collected_data)\n",
    "    return json_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2039446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url, headers=headers, data=json.dumps(data), stream=True)\n",
    " \n",
    "\n",
    "stream_response(response, printer=printer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eb74c4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.anthropic.com/v1/messages\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"x-api-key\":  os.environ.get('ANTHROPIC_API_KEY'),\n",
    "    \"anthropic-version\": \"2023-06-01\",\n",
    "}\n",
    "\n",
    "def _adapt_tools_for_anthropic(functions):\n",
    "    \"\"\"slightly different dialect of function wrapper - rename parameters to input_schema\"\"\"\n",
    "    def _rewrite(d):\n",
    "        return {\n",
    "            'name' : d['name'],\n",
    "            'input_schema': d['parameters'],\n",
    "            'description': d['description']\n",
    "        }\n",
    "\n",
    "    return [_rewrite(d) for d in functions or []]\n",
    "\n",
    "data = {\n",
    "    \"model\": \"claude-3-5-sonnet-20241022\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"messages\": [\n",
    "         {\"role\": \"user\", \"content\": \"What's the weather in Paris tomorrow?\"}\n",
    "         #{\"role\": \"user\", \"content\": \"What's capital of France\"}\n",
    "    ],\n",
    "    \"stream\" : True,\n",
    "    \"tools\": _adapt_tools_for_anthropic(fns)\n",
    "}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915ecf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "08bb01b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import json\n",
    "\n",
    "def stream_anthropic_response(r, printer=None):\n",
    "    collected_data = None\n",
    "    tool_args = {}  # {tool_id: {index: aggregated_args}}\n",
    "    content_text = \"\"  # To accumulate non-tool content\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    observed_tool_call = False\n",
    "\n",
    "    event_type = None\n",
    "    current_content_index = None\n",
    "    content_block_type = None\n",
    "    idnex = None\n",
    "    for line in r.iter_lines():\n",
    "        if line:\n",
    "            decoded_line = line.decode(\"utf-8\")\n",
    "            if decoded_line[:6] == 'event:':\n",
    "                event_type = decoded_line.replace(\"event: \", \"\").strip()\n",
    "                continue\n",
    "            else:\n",
    "                decoded_line = decoded_line.replace(\"data: \", \"\").strip()\n",
    "\n",
    "            if decoded_line and decoded_line != \"[DONE]\":\n",
    "                try:\n",
    "                    json_data = json.loads(decoded_line)\n",
    "                    event_type = json_data.get(\"type\")\n",
    "                    \n",
    "                    #print(json_data)\n",
    "                    \n",
    "                    # Handle message start: Initialize structure from the first message\n",
    "                    if event_type == \"message_start\":\n",
    "                        collected_data = dict(json_data['message'])\n",
    "                        input_tokens = collected_data['usage']['input_tokens']\n",
    "    \n",
    "                    elif event_type == \"content_block_start\":\n",
    "                        content_block_type = json_data['content_block']['type']\n",
    "                        print(content_block_type)\n",
    "                        index = json_data['index']\n",
    "                        if content_block_type == 'tool_use':\n",
    "                            tool_content = json_data['content_block']\n",
    "                            tool_content['partial_json'] = ''\n",
    "                            tool_args[index]  = tool_content\n",
    "                    # Handle content block deltas with text updates\n",
    "                    elif event_type == \"content_block_delta\" and content_block_type != 'tool_use':\n",
    "                        content_type = json_data[\"delta\"].get(\"type\")\n",
    "                        if content_type == \"text_delta\":\n",
    "                            text = json_data[\"delta\"].get(\"text\", \"\")\n",
    "                            content_text += text\n",
    "                            if printer:\n",
    "                                printer(text)\n",
    "\n",
    "                    # Handle tool calls and match args using the index\n",
    "                    elif event_type == \"content_block_delta\" and content_block_type == 'tool_use':\n",
    "                        tool_input = json_data[\"delta\"].get(\"partial_json\")\n",
    "                        if tool_input:\n",
    "  \n",
    "                            \"\"\"TODO store the aggregated json per tool and add at the end into this structure\n",
    "                            example\n",
    "                            {'type': 'tool_use',\n",
    "                           'id': 'toolu_01GV5rqVypHCQ6Yhrfsz8qhQ',\n",
    "                           'name': 'get_weather',\n",
    "                           'input': {'city': 'Paris', 'date': '2024-01-16'}}],\n",
    "                            \"\"\"\n",
    "                            tool_args[index]['partial_json'] += tool_input\n",
    "                            \n",
    "                            if not observed_tool_call:\n",
    "                                observed_tool_call = True\n",
    "                                if printer:\n",
    "                                    printer(f\"Tool call for {tool_args[index]}\")\n",
    "\n",
    "                    # Handle message delta and stop reason at the end\n",
    "                    elif event_type == \"message_delta\":\n",
    "                        output_tokens = json_data.get(\"usage\", {}).get(\"output_tokens\", 0)\n",
    "                        collected_data['stop_reason'] = json_data.get('stop_reason')\n",
    "                        collected_data['stop_sequence'] = json_data.get('stop_sequence')\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    pass  # Handle incomplete JSON chunks\n",
    "\n",
    "    # Aggregate content and tool calls into the final structure\n",
    "    collected_data['content'] = [{\"type\": \"text\", \"text\": content_text}, \n",
    "                                *[list(tool_args.values())]]\n",
    "    # Update usage tokens\n",
    "    collected_data['usage']['input_tokens'] = input_tokens\n",
    "    collected_data['usage']['output_tokens'] = output_tokens\n",
    "\n",
    "    return collected_data\n",
    "\n",
    "\n",
    "# Example usage (assuming you have a valid `response` object from requests)\n",
    "response = requests.post(url, headers=headers, json=data, stream=True)\n",
    "result = stream_anthropic_response(response, printer)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c7ce97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e3ed0f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def stream_google_gemini_response():\n",
    "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:streamGenerateContent?alt=sse&key=${os.environ.get('GEMINI_API_KEY')}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"parts\": [{\"text\": \"Write a cute story about cats.\"}]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Send the request with stream=True to enable streaming\n",
    "    with requests.post(url, json=data, headers=headers, stream=True) as response:\n",
    "        # Check for a successful response\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        # Initialize a variable to hold the current parts of the text\n",
    "        current_text_parts = []\n",
    "\n",
    "        # Process the streamed response\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                # Each chunk of data is prefixed with 'data: ', so we strip that part\n",
    "                line = line.decode(\"utf-8\").strip()\n",
    "                if line.startswith(\"data: \"):\n",
    "                    # Remove 'data: ' and parse the JSON\n",
    "                    json_data = json.loads(line[len(\"data: \"):])\n",
    "\n",
    "                    # Extract the text parts from the streamed chunk\n",
    "                    candidates = json_data.get(\"candidates\", [])\n",
    "                    for candidate in candidates:\n",
    "                        parts = candidate.get(\"content\", {}).get(\"parts\", [])\n",
    "                        for part in parts:\n",
    "                            text = part.get(\"text\", \"\")\n",
    "                            # Collect the text content and print it\n",
    "                            current_text_parts.append(text)\n",
    "                            print(text)  # Print each part as it comes in\n",
    "\n",
    "                    # If the response indicates completion (e.g., 'finishReason': 'STOP'), exit\n",
    "                    finish_reason = candidate.get(\"finishReason\")\n",
    "                    if finish_reason == \"STOP\":\n",
    "                        print(\"Streaming finished.\")\n",
    "                        break\n",
    "\n",
    "        # Combine all the collected text parts into the final result\n",
    "        final_output = \" \".join(current_text_parts)\n",
    "        return final_output\n",
    "\n",
    "# Example usage\n",
    "final_story = stream_google_gemini_response()\n",
    "final_story\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe683c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
