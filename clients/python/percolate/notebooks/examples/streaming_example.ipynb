{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percolate API Streaming Example\n",
    "\n",
    "This notebook demonstrates how to properly consume streaming responses from the Percolate API in a Jupyter notebook environment.\n",
    "\n",
    "The key to getting true streaming character-by-character display is to use the `requests` library with `stream=True` instead of TestClient, and process each chunk as it arrives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from IPython.display import clear_output, display\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the path so we can import percolate\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup API URL\n",
    "\n",
    "Change this to match your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API URL - adjust as needed for your environment\n",
    "API_URL = \"http://localhost:8000/chat/completions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Using TestClient (problematic for streaming)\n",
    "\n",
    "This approach will show the issue you're experiencing - chunks appear all at once rather than character-by-character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.testclient import TestClient\n",
    "from fastapi import FastAPI\n",
    "import percolate.api.routes.chat.router as chat_router\n",
    "\n",
    "# Create a simple FastAPI app for testing\n",
    "app = FastAPI()\n",
    "app.include_router(chat_router.router, prefix=\"/chat\")\n",
    "\n",
    "# Create test client\n",
    "client = TestClient(app)\n",
    "\n",
    "# Request payload\n",
    "payload = {\n",
    "    \"model\": \"gpt-4o-mini\",  # Change to match your available model\n",
    "    \"prompt\": \"Write a poem about Paris in 4 lines\",\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0.7,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "# Make streaming request with TestClient\n",
    "print(\"Using TestClient (chunks appear all at once):\")\n",
    "with client.stream(\"POST\", \"/chat/completions\", json=payload) as response:\n",
    "    for chunk in response.iter_text():\n",
    "        print(f\"Received chunk: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Using requests with stream=True (recommended)\n",
    "\n",
    "This approach properly shows incremental updates character-by-character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_with_requests():\n",
    "    \"\"\"Example of properly consuming a streaming response with requests.\"\"\"\n",
    "    \n",
    "    # Request payload\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",  # Change to match your available model\n",
    "        \"prompt\": \"Write a poem about Paris in 4 lines\",\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stream\": True\n",
    "    }\n",
    "    \n",
    "    # Make the request with streaming enabled\n",
    "    print(\"Sending request...\")\n",
    "    response = requests.post(\n",
    "        API_URL,\n",
    "        json=payload,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        stream=True  # This is critical for streaming!\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return\n",
    "    \n",
    "    print(\"\\nStreaming response (character by character):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Accumulated text for display\n",
    "    full_text = \"\"\n",
    "    \n",
    "    # Process each line in the stream\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                # Decode the line from bytes to string\n",
    "                line_text = line.decode('utf-8')\n",
    "                \n",
    "                # Handle SSE format if it starts with 'data: '\n",
    "                if line_text.startswith('data: '):\n",
    "                    line_text = line_text[6:]  # Remove 'data: ' prefix\n",
    "                \n",
    "                # Skip the [DONE] marker\n",
    "                if line_text == \"[DONE]\":\n",
    "                    continue\n",
    "                \n",
    "                # Parse the JSON\n",
    "                data = json.loads(line_text)\n",
    "                \n",
    "                # Extract content based on the response format\n",
    "                content = \"\"\n",
    "                if \"choices\" in data and data[\"choices\"]:\n",
    "                    # Standard OpenAI format\n",
    "                    if \"delta\" in data[\"choices\"][0]:\n",
    "                        delta = data[\"choices\"][0][\"delta\"]\n",
    "                        if \"content\" in delta:\n",
    "                            content = delta[\"content\"]\n",
    "                    # Our canonical format\n",
    "                    elif \"text\" in data[\"choices\"][0]:\n",
    "                        content = data[\"choices\"][0][\"text\"]\n",
    "                \n",
    "                # Add to full text and display incrementally\n",
    "                if content:\n",
    "                    full_text += content\n",
    "                    # Display each character with a slight delay to demonstrate streaming\n",
    "                    for char in content:\n",
    "                        # Print character by character with flush to show incremental updates\n",
    "                        print(char, end='', flush=True)\n",
    "                        time.sleep(0.01)  # Small delay to make the streaming visible\n",
    "            \n",
    "            except json.JSONDecodeError:\n",
    "                # Handle non-JSON data if any\n",
    "                print(f\"Raw: {line.decode('utf-8')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(f\"Complete text: {full_text}\")\n",
    "\n",
    "# Run the example\n",
    "stream_with_requests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Advanced Visualization with IPython\n",
    "\n",
    "This method uses IPython's display capabilities to show a more visually appealing incremental update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "def stream_with_ipython_display():\n",
    "    \"\"\"Example with IPython's display capabilities for nice visualization.\"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",  # Change to match your available model\n",
    "        \"prompt\": \"Write a poem about Paris in 4 lines, highlighting the beauty of the Eiffel Tower.\",\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7,\n",
    "        \"stream\": True\n",
    "    }\n",
    "    \n",
    "    print(\"Sending request...\")\n",
    "    response = requests.post(\n",
    "        API_URL,\n",
    "        json=payload,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return\n",
    "    \n",
    "    print(\"Streaming response with IPython display:\")\n",
    "    \n",
    "    # Accumulated text\n",
    "    full_text = \"\"\n",
    "    new_text = \"\"\n",
    "    \n",
    "    # Process each line\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            try:\n",
    "                line_text = line.decode('utf-8')\n",
    "                if line_text.startswith('data: '):\n",
    "                    line_text = line_text[6:]\n",
    "                if line_text == \"[DONE]\":\n",
    "                    continue\n",
    "                    \n",
    "                data = json.loads(line_text)\n",
    "                \n",
    "                content = \"\"\n",
    "                if \"choices\" in data and data[\"choices\"]:\n",
    "                    if \"delta\" in data[\"choices\"][0]:\n",
    "                        delta = data[\"choices\"][0][\"delta\"]\n",
    "                        content = delta.get(\"content\", \"\")\n",
    "                    elif \"text\" in data[\"choices\"][0]:\n",
    "                        content = data[\"choices\"][0][\"text\"]\n",
    "                \n",
    "                if content:\n",
    "                    # Add to accumulated text\n",
    "                    full_text += content\n",
    "                    new_text += content\n",
    "                    \n",
    "                    # Display with highlighting for new text\n",
    "                    display_text = full_text[:-len(new_text)] + f\"<span style='color:red;font-weight:bold'>{new_text}</span>\"\n",
    "                    clear_output(wait=True)\n",
    "                    display(HTML(f\"<p>{display_text}</p>\"))\n",
    "                    \n",
    "                    # Reset new text after a short delay\n",
    "                    if len(new_text) > 10 or \" \" in new_text:\n",
    "                        time.sleep(0.3)\n",
    "                        new_text = \"\"\n",
    "                    else:\n",
    "                        time.sleep(0.05)\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "    \n",
    "    # Final display without highlighting\n",
    "    clear_output(wait=True)\n",
    "    display(HTML(f\"<p>{full_text}</p>\"))\n",
    "\n",
    "# Run the advanced example\n",
    "stream_with_ipython_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why TestClient Doesn't Show Character-by-Character Streaming\n",
    "\n",
    "The TestClient in FastAPI is designed for testing endpoints, not for real-world consumption of streaming APIs. It doesn't process streams the same way that a real HTTP client would.\n",
    "\n",
    "When you use TestClient:\n",
    "1. It collects chunks as defined by the server's `yield` statements\n",
    "2. It doesn't break these down further into smaller increments\n",
    "3. It presents each complete chunk as a single unit\n",
    "\n",
    "For true streaming behavior, always use `requests` with `stream=True` in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "To get proper streaming in your application:\n",
    "\n",
    "1. Use the real `requests` library, not TestClient\n",
    "2. Always set `stream=True` in your requests call\n",
    "3. Process chunks incrementally using `response.iter_lines()`\n",
    "4. Display content character-by-character with proper flushing\n",
    "\n",
    "This approach will show true incremental streaming behavior, just like the real API would deliver."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
