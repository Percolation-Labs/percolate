{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50635797",
   "metadata": {},
   "source": [
    "# Working with Message Stack payloads \n",
    "- there are currently three support schemes; openai, google, anthropic\n",
    "- these have slightyl different payload structure for tool calls and responses in particular\n",
    "- generally, we want to ACK a tool call with an id and follow it with a response in the message stack\n",
    "- anthropic has a tool block and google has a functionResponse while open AI is easier with just the typically message with role and content\n",
    "- we can test read message stacks as instructions from the database in different contexts\n",
    "    - for a user question; trivial \n",
    "    - for a tool request with tool stack\n",
    "    - for agents that provider system prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ffeeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import percolate as p8\n",
    "from percolate.models.p8 import AIResponse\n",
    "from percolate.services import PostgresService\n",
    "from percolate.services.llm.LanguageModel import request_anthropic,request_google,request_openai\n",
    "from pydantic import BaseModel, model_validator, Field\n",
    "import typing\n",
    "import uuid\n",
    "\n",
    "pg = PostgresService()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3cf688",
   "metadata": {},
   "source": [
    "### illustrate that in each scheme we read messages that are ready for that scheme and functions too\n",
    "- there is a corresponding database request_x that reads data in the same way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873008f0",
   "metadata": {},
   "source": [
    "## The first test should take a singel turn example id\n",
    "- at this point you have created a request with any scheme using e.g. percolate_with_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8486e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_session_id = '8c51c161-7ac8-db55-68ec-7255ea4983e0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78a609ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "goo_mm =  [d for d in pg.execute(f\"\"\" select * from p8.get_google_messages('{test_session_id}') \"\"\")[0]['messages']]  \n",
    "fns =  [d for d in pg.execute(f\"\"\" select * from p8.get_tools_by_name(ARRAY['get_pet_findByStatus'],'google') \"\"\")[0]['get_tools_by_name']]  \n",
    "request_google(goo_mm,fns).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1171d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_mm = [d for d in pg.execute(f\"\"\" select * from p8.get_anthropic_messages('{test_session_id}') \"\"\")[0]['messages']]\n",
    "fns =  [d for d in pg.execute(f\"\"\" select * from p8.get_tools_by_name(ARRAY['get_pet_findByStatus'],'anthropic') \"\"\")[0]['get_tools_by_name']]  \n",
    "request_anthropic(ant_mm,fns).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f5b3cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = [d for d in pg.execute(f\"\"\" select * from p8.get_canonical_messages('{test_session_id}') \"\"\")[0]['messages']]\n",
    "fns =  [d for d in pg.execute(f\"\"\" select * from p8.get_tools_by_name(ARRAY['get_pet_findByStatus']) \"\"\")[0]['get_tools_by_name']]  \n",
    "request_openai(mm,fns).json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e596c",
   "metadata": {},
   "source": [
    "## Longer turn tests\n",
    "- make sure function calls and responses are paired properly \n",
    "- test injecting in new quuestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9797ca",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219971a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e917237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import percolate as p8\n",
    "from percolate.models.p8 import AIResponse\n",
    "from percolate.services import PostgresService\n",
    "from percolate.services.llm.LanguageModel import request_anthropic,request_google,request_openai, LanguageModel,CallingContext\n",
    "from pydantic import BaseModel, model_validator, Field\n",
    "import typing\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1b5d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create a simple printer that we can pass down\"\"\"\n",
    "def printer(text):\n",
    "    \"\"\"streaming output\"\"\"\n",
    "    print(text, end=\"\", flush=True)  \n",
    "context = CallingContext(streaming_callback=printer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f94e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fns =[{\n",
    "  \"name\": \"get_weather\",\n",
    "  \"description\": \"Get the weather forecast for a specific city and date\",\n",
    "  \"parameters\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"city\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The city for which to get the weather forecast\"\n",
    "      },\n",
    "      \"date\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The date for the weather forecast (YYYY-MM-DD)\"\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\"city\", \"date\"]\n",
    "  }\n",
    "}]\n",
    "\n",
    "#this maps to tools by just wrapping\n",
    "tools = [{'type': 'function', 'function': f} for f in fns]\n",
    "\n",
    "models = ['gpt-4o-mini', 'deepseek-chat', 'claude-3-5-sonnet-20241022', 'gemini-1.5-flash']\n",
    "#we can test each of the models ^ \n",
    "#we keep one in eacn scheme - open ai, google and ahtnropic but we also use a second non open ai that uses the same schema to test consistency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e1c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-4o-mini', 'deepseek-chat', 'claude-3-5-sonnet-20241022', 'gemini-1.5-flash']\n",
    "#we can test each of the models ^ \n",
    "#we keep one in eacn scheme - open ai, google and ahtnropic but we also use a second non open ai that uses the same schema to test consistency\n",
    "\n",
    "model = LanguageModel(models[0])\n",
    "#1. no stream no tool\n",
    "#model.ask(\"What is the capital of ireland\", functions=fns)\n",
    "# 2 stream no tool - streaming just add a context\n",
    "model.ask(\"What is the capital of ireland\", functions=fns, context=context, debug_response=False)\n",
    "# 3 no stream and tool\n",
    "#model.ask(\"What is the weather in paris tomorrow\", functions=fns)\n",
    "# 4 stream and tool\n",
    "#model.ask(\"What is the weather in paris tomorrow\", functions=fns,  context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb928a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33957fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(models[1])\n",
    "#1. no stream no tool\n",
    "#model.ask(\"What is the capital of ireland\", functions=fns)\n",
    "# 2 stream no tool - streaming just add a context\n",
    "#model.ask(\"What is the capital of ireland form your world knowledge\", functions=fns, context=context, debug_response=False)\n",
    "# 3 no stream and tool\n",
    "#model.ask(\"What is the weather in paris tomorrow\", functions=fns)\n",
    "# 4 stream and tool\n",
    "#model.ask(\"What is the weather in paris tomorrow\", functions=fns,  context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526fd1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(models[2])\n",
    "#1. no stream no tool\n",
    "#model.ask(\"What is the capital of ireland\", functions=fns)\n",
    "# 2 stream no tool - streaming just add a context\n",
    "#model.ask(\"What is the capital of ireland from your world knolwedge\", functions=fns, context=context, debug_response=False)\n",
    "# 3 no stream and tool\n",
    "#model.ask(\"What is the weather in paris tomorrow\", functions=fns)\n",
    "# 4 stream and tool\n",
    "model.ask(\"What is the weather in paris tomorrow\", functions=fns,  context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "871d5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(models[3])\n",
    "#1. no stream no tool\n",
    "#model.ask(\"What is the capital of ireland based on world knowledge\", functions=fns)\n",
    "# 2 stream no tool - streaming just add a context\n",
    "#model.ask(\"What is the capital of ireland from your world knolwedge\", functions=fns, context=context, debug_response=False)\n",
    "# 3 no stream and tool\n",
    "#model.ask(\"What is the weather in paris tomorrow if tomorrow is the 4th of feb 2024\", functions=fns)\n",
    "# 4 stream and tool\n",
    "model.ask(\"What is the weather in paris tomorrow if tomorrow is the 4th of feb 2024\", functions=fns,  context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe683c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
