{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2630d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from IPython.display import Markdown\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4abbda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['P8_TEST_BEARER_TOKEN'] = os.environ.get('EEPIS_PW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c6a645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import percolate as p8\n",
    "from percolate.models import Session, SessionResources,Resources,Project, Task\n",
    "\n",
    "models = [Session, SessionResources,Resources,Project, Task]\n",
    "\n",
    "def update_models():\n",
    "    for m in models:\n",
    "        print(m)\n",
    "        p8.repository(m).register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feff9726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from percolate.api.examples.client_examples import upload_file_example, upload_uri_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0285dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload_uri_example('https://en.wikipedia.org/wiki/Breed_standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520625af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload_file_example(\"/Users/sirsh/Downloads/example.txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "275f737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload_file_example(\"/Users/sirsh/Downloads/greatmeetingeepisdocuments/Technical Roadmap - Ever-Evolving Personal Intelligence System (EEPIS)  (11).pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c76f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from percolate.services.llm.utils import print_openai_delta_content\n",
    "from percolate.services.llm import CallingContext\n",
    "ctx = CallingContext(streaming_callback=print_openai_delta_content)\n",
    "\n",
    "a = p8.Agent(Resources, allow_help=False)\n",
    "\n",
    "#m = a(\"describe the architecture of eepis - are there any other documents we should look up\")\n",
    "# m = a(\"Do you know anything abou the brain bridge - what privacy features does it have\",  language_model='grok-2-latest')\n",
    "# Markdown(m)\n",
    "\n",
    "# for data in a.iter_lines(\"what is the capital of ireland\",audit=False):\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0b1ea9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for data in a.iter_lines(\"Do you know anything abou the brain bridge\",audit=False,context=ctx):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f76f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.messages.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5585aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.get_entities('Chat Log36.pdf (116)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbfdc2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p8.repository(Resources).search(\" describe the architecture of eepis \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b8de69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.streaming_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"locally log into google drive and list files - in your browser http://127.0.0.1:5008/auth/google/login\"\"\"\n",
    "\n",
    "\"\"\"callback saves a token for testing\"\"\"\n",
    "\n",
    "\n",
    "from percolate.services.dox.gs import test_token, list_files_as_chunked_resources,list_all_drive_files_flat, _extract_word_doc_text\n",
    "from pathlib import Path\n",
    "TOKEN_PATH = Path.home() / '.percolate' / 'auth' / 'google' /  'token'\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(TOKEN_PATH, 'r') as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "#test_token(d['access_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d316e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from percolate.services import PostgresService\n",
    "\n",
    "# pg = PostgresService()\n",
    "# eg = [u['name'] for u in pg.execute(\"\"\"select distinct name from p8.\"Resources\" \"\"\")]\n",
    "# eg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff5c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from percolate.models import Resources\n",
    "for chunk in tqdm(list_files_as_chunked_resources(d['access_token'])):\n",
    "    try:\n",
    "        p8.repository(Resources).update_records(chunk)\n",
    "    except Exception as ex:\n",
    "        print(chunk.name, ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd02db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = list(list_all_drive_files_flat(d['access_token']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51efcd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a,b = _extract_word_doc_text(d['access_token'], '1BwJd4iefpM0HLBXHTG-A_YNbyHj3joMu', 'test') #MS WORD\n",
    "a,b = _extract_word_doc_text(d['access_token'], '1Jpt_jbYAJHldNQBxpRfDdQw5liJ5KIQK', 'test') #MS WORD\n",
    "#a,b = _extract_pdf_text(d['access_token'], '1DkYoOthi-COYEZo8HBBPsoH8ThdMScVf', 'test') # PDF\n",
    "b\n",
    "#read_doc_as_text(d['access_token'],dict(pd.DataFrame(files).iloc[40:50].iloc[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb22cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = '##\\n![](images/image_1.png)\\n\\n## **Figure 1: System Architecture Overview**\\n\\n _Figure 1._ High-level architecture of the Ever-Evolving Personal\\nIntelligence System (EEPIS). Major functional components – **Memory Engine\\n(101)** , **Privacy/Data Management Subsystem (102)** , **Context Recognition\\n& User Interface Subsystem (103)**, **Automation & Decision Engine (104)**,\\nand **Analytics & Insight Module (105)** – are illustrated in a block diagram,\\nalong with their interactions.\\n\\nMultimodal **sensor inputs** (e.g. camera, microphone, biosensors, location)\\nfeed into the system, first encountering a **Privacy Filter** (part of\\nsubsystem 102) that enforces user-defined privacy rules. Allowed data flows\\ninto the **Multimodal Analysis & Context Tagging** module (part of Memory\\nEngine 101) for processing (speech-to-text, image recognition, etc.) and\\ncontext annotation. Processed information is then stored in an **episodic\\nMemory store** (encrypted local database, with secure cloud backup) and\\nindexed in the **Memory Index/Search** component for later retrieval.\\nSimultaneously, recognized patterns can trigger the **Automation Engine\\n(104)** for real-time suggestions or actions. The **Analytics/Insight Module\\n(105)** performs cross-domain analysis on stored data to generate higher-level\\ninsights, feeding results back to the user. User interactions occur through\\nthe **User Interface (103)** – e.g. AR displays, notifications, or voice –\\nwhich both presents recall information and handles user input (queries,\\ncommands), closing the feedback loop. This figure aligns with the disclosure’s\\ndescription of an integrated system comprising these core subsystems working\\ntogether.\\n\\n![](images/image_2.png)\\n\\n## **Figure 2: Multi-Modal Data Capture Process**\\n\\n _Figure 2._ Flowchart of the **multimodal data capture and memory storage\\nprocess** , corresponding to Workflow 1 in the provisional disclosure.\\nMultiple sensor inputs (audio, video, biometrics, location, etc.) concurrently\\nstart the flow, funneling into a privacy check. At the **Privacy Filter\\nCheck** , a decision is made: if the incoming data is permitted under user\\nprivacy settings, processing continues; if **not allowed** , the data is\\ndiscarded (not recorded). Allowed data then undergoes **Data Processing &\\nAnalysis**, where the system converts raw inputs into structured information\\n(e.g. audio to text, image to recognized entities) as described in the\\ndisclosure’s Step 3. Next, the flow **attaches context** (time, location,\\nparticipants, etc.) to the data (Step 4), enriching it with situational\\nmetadata. An **Event Formation decision** follows: the system checks if the\\ndata belongs to an existing event context (ongoing session) – if **yes** , it\\nadds the data to that event; if **no** , it creates a new event container\\u200b.\\nAfter updating or creating an event, the system **categorizes and\\nprioritizes** the event (Step 6) based on content and context (e.g. tag as\\n“Work meeting” or determine importance). Next, the **Secure Storage\\nSelection** step decides where to store the event data (local device, cloud\\nbackup, and/or decentralized storage) according to its sensitivity (Step 7).\\nIn this flowchart, a decision “Sensitive data?” splits the path: highly\\nsensitive data is kept only in secure local/decentralized storage, while other\\ndata is also backed up to the cloud (all data is encrypted in either case).\\nAfter storing, the system **updates the Memory Index** (Step 8) to make the\\nnew event searchable\\u200b. A **Post-processing** step (Step 9) then runs analytics\\non the event – e.g. updating user stats or triggering immediate follow-ups\\n(such as reminders for tasks noted. Finally, the flow ends with the data\\nsecurely stored and indexed, and the system returns to monitoring for the next\\ninput. (This process repeats continuously for each new data stream\\u200b, ensuring\\nall relevant user experiences are captured in memory.)\\n\\n![](images/image_3.png)**Figure\\n3: Contextual Memory Recall Process**\\n\\n _Figure 3._ Flowchart of the **contextual memory recall** process (Workflow\\n2), where EEPIS retrieves and presents stored information to the user based on\\na query or context cue. The process begins with a **Recall Trigger** – either\\nan explicit user query (e.g. the user asks a question via voice or text) or an\\nimplicit context cue (e.g. the system recognizes a person’s face via AR, or\\ndetects the user’s location) that prompts a memory search. Upon such a\\ntrigger, the system **gathers context** (Step 2): it collects relevant\\nsituational details like who is present, where the user is, the current time,\\nand recent events or interactions related to the cue. Using this rich context,\\nthe system **formulates a memory search query** (Step 3) – essentially\\ntranslating the situation into keywords or parameters for querying the memory\\nindex. Next, the **Memory Index is searched** (Step 4) for matching events or\\ninformation. A critical decision follows: **“Memory found?”** If no relevant\\nmemory is found, the system will gracefully handle it (e.g. notify the user\\nthat no information was found for that query/context)\\u200b. If one or more matches\\nare found, the system proceeds to **rank the results** by relevance (Step 5),\\nfor example favoring more recent or significant events as described in the\\ndisclosure\\u200b. The top result is then used in Step 6: the system **generates a\\nconcise summary** of that memory (key details like when it occurred, the\\ncontext, any important promises or data). In Step 7, the system **delivers the\\nrecall summary to the user** via the appropriate interface – for instance,\\noverlaying text in the user’s AR glasses view, speaking the summary aloud, or\\nshowing a notification on a device. This allows the user to instantly recall\\nthe needed information in context (e.g. seeing a brief summary about a person\\nthey just encountered). The flow then ends with the user informed; if the user\\nneeds more detail, they could trigger another recall (not shown here, but\\nmentioned as a possible follow-up in the disclosure). Throughout this process,\\nEEPIS ensures that only the authenticated user perceives the recalled\\ninformation (protecting privacy in shared environments)\\u200b, and it continuously\\nlinks new experiences to past memories (for example, the current interaction\\nitself would be recorded for future recall).\\n\\n![](images/image_4.png)\\n\\n## **Figure 4: Proactive Suggestion & Automation Process**\\n\\n _Figure 4._ Flowchart of the **proactive suggestion and automation** process\\n(Workflow 3), in which EEPIS monitors the user’s context and acts autonomously\\nto assist, subject to user preferences\\u200b. The loop begins with the system\\n**continuously monitoring the user’s context** (time, location, activity,\\nsensor data, etc.) for any conditions that might warrant assistance (Step 1).\\nThis is depicted as a continuous background loop (“Monitor user context\\ncontinuously”) feeding into a decision point. The first decision, **“Trigger\\ncondition met?”** , evaluates whether the current context matches any\\npredefined rule or learned pattern for proactive action. If no conditions are\\nmet, the system simply continues monitoring (looping back with no action). If\\n**yes** , then a trigger event is recognized (e.g. the user is driving and an\\nunread text is detected, or it’s nearly a meeting time with a conflict on the\\ncalendar) and the system proceeds to **formulate a suggestion or action plan**\\n(Step 3). In this formulation step, the AI decides _what_ assistance to offer\\nand prepares the details (for example, deciding to draft a meeting reschedule\\nproposal and identifying a better time slot). Next, the system checks\\n**“Approval required?”** (Step 4): based on user-defined policies and the\\nnature of the action, the AI determines if it can execute the action\\nautonomously or if it should ask the user first\\u200b. If no approval is needed\\n(for benign or pre-approved tasks), the flow skips ahead to execute the action\\ndirectly. If **yes** , the system will **notify the user with the suggestion\\nand request confirmation** (Step 5). This is illustrated by the “Notify user…”\\nstep, which might be an AR prompt, a phone notification, etc., describing the\\nsuggested action and offering accept/decline options. Then a decision **“User\\napproved?”** is made (Step 5 continued): if the user approves the suggestion\\n(or if it was auto-approved), the flow continues; if the user declines or\\nignores it, the system cancels the action (no changes are made)\\u200b. When\\napproved, the system **executes the action** on the user’s behalf (Step 6) –\\nfor example, sending a message, adjusting a device setting, or ordering a\\nservice – by interfacing with the relevant external APIs or devices. After\\nexecution, the system enters a post-action phase: it **logs the outcome and\\ncollects feedback** (Step 7), noting success or failure and any relevant\\nresponses. It then **updates its learning models** (Step 8) based on the\\nresult – reinforcing actions that were successful and approved, or adjusting\\nits behavior if the suggestion was declined or the action failed\\u200b. Finally,\\nthe loop returns to context monitoring (Step 9), with the system now a bit\\nsmarter about the user’s preferences (for instance, it might not prompt again\\nfor the same suggestion if the user consistently declined it). This proactive\\nworkflow illustrates how EEPIS “thinks ahead,” yet always keeps the user in\\ncontrol through decision checkpoints, as described in the patent disclosure.\\n\\n![](images/image_5.png)\\n\\n## **Figure 5: Federated Learning & Model Update Cycle**\\n\\n _Figure 5._ Diagram of the **federated learning and model update** process in\\nEEPIS, an additional key subsystem that ensures the AI model improves over\\ntime without centralized data collection\\u200b. This cycle begins with the user’s\\ndevice performing **local model training** on new personal data (e.g. each\\nnight when the device is idle) – the AI model refines itself using the latest\\nuser-specific data (messages, behavior patterns, etc.) stored in the personal\\nmemory. Once local training is completed, the device computes a **model\\nupdate** (the difference between the updated model and the previous model) and\\napplies differential privacy (DP) noise to that update to **anonymize it** ,\\nensuring no private details can be reconstructed. The device then **uploads\\nthis anonymized model update** to the EEPIS cloud aggregator. In the **cloud\\naggregation step** , the server collects model updates from many users’\\ndevices (each update is small and contains no raw personal data)\\u200b. The server\\n**aggregates and combines** these updates – for example, by averaging the\\nweight differences – to compute an **improved global model** that incorporates\\nlearnings from across the user community\\u200b. Next, the server **distributes the\\nglobal model update** back to each device. The user’s device receives this\\n**global update** and **merges it into its local model** , thereby benefiting\\nfrom collective learning (e.g. adapting to new slang or scenarios observed in\\nothers’ data) without ever exposing individual data. This cycle (local train →\\nupload update → aggregate → download update) repeats periodically, enabling\\nthe personal AI to get smarter both individually and as part of a federated\\nnetwork. Throughout the process, **user data remains private** – only abstract\\nmodel parameters are shared – which aligns with EEPIS’s privacy-first design\\n(no raw timeline of events ever leaves the device). This figure corresponds to\\nthe Federated Learning subsystem described in the disclosure and demonstrates\\nhow EEPIS continuously self-improves while upholding user data privacy.\\n\\n![](images/image_6.png)\\n\\n### **Figure 6: Brain-Computer Interface (BCI) Integration**\\n\\nThe diagram above shows how neural inputs would flow through EEPIS,\\nintegrating a future **brain-computer interface** into the system’s workflow.\\nA BCI device (e.g. EEG headset) captures raw neural signals from the user,\\nwhich EEPIS then processes to detect any meaningful intent or command. At a\\ndecision point, the system interprets whether the neural signal corresponds to\\na deliberate user command/intent or is simply contextual data (such as an\\nemotional state). If a concrete command is recognized (e.g. the user “thinks”\\na query or instruction), EEPIS routes it to the appropriate module – for\\ninstance, recalling a memory or executing an action – enabling truly hands-\\nfree interaction via thought\\u200b. The system then carries out the command and\\nprovides feedback to the user (perhaps via an AR display or audio), completing\\nthe thought-triggered action. If no explicit command is detected (i.e. the BCI\\ninput reflects context like stress or concentration level), the neural data is\\nused to **update the user’s context** within EEPIS. For example, a detected\\nhigh focus level might prompt EEPIS to hold off non-urgent notifications,\\nleveraging the user’s cognitive state to adjust its behavior\\u200b. In either case,\\nthe BCI integration allows EEPIS to respond to neural cues – proactively\\nassisting based on intent or adapting to the user’s mood – which is a novel,\\nforward-looking feature of the system.\\n\\n![](images/image_7.png)\\n\\n### **Figure 7: Privacy & Data Governance**\\n\\nThis black-and-white flowchart illustrates how EEPIS handles user data with a\\nprivacy-first design. The process starts with new data captured from sensors\\nor user input, immediately subjected to a privacy check. If the data is\\ndisallowed by the user’s privacy preferences, it is discarded and not stored.\\nAllowed data continues through processing and contextual tagging, then a\\n**sensitivity-based storage decision** is made: highly sensitive information\\nis kept only in secure local storage, whereas other data is encrypted and\\nbacked up to the cloud for redundancy\\u200b. After storage, the system updates its\\nmemory index to enable quick search and recall of this data. The diagram also\\nshows a user-initiated deletion sequence (the “Forget” command): at any later\\ntime, the user can choose to delete stored information. Upon such a request,\\nEEPIS will locate all copies of the targeted data across local or cloud stores\\nand permanently erase them, updating indexes and logs accordingly. This\\nensures compliance with data governance policies (e.g. the “right to be\\nforgotten”) and gives the user full control over what is retained or removed\\nin their personal data vault.\\n\\n![](images/image_8.png)\\n\\n### **Figure 8: Full-System Overview**\\n\\nThis high-level flow diagram summarizes the entire EEPIS data flow from input\\nto output, highlighting how the system senses, decides, learns, and acts. It\\nbegins with **multimodal sensor inputs** (camera, microphone, biosensors,\\netc.) being collected and fed into a Privacy Filter. The Privacy/Permissions\\ncheck ensures only authorized data passes through – any data violating user-\\nset rules is blocked and discarded\\u200b. Approved data is then contextualized\\n(e.g. tagged with time, location, recognized entities) and stored in the\\nepisodic memory repository. From the memory, two parallel pathways unfold: (1)\\na real-time Automation & Decision Engine monitors incoming information for\\ntriggers to immediately assist the user, and (2) an Analytics module performs\\ndeeper batch analysis to derive insights or trends from accumulated data. If a\\ntrigger condition is detected (for example, a pattern that matches a user’s\\npreset rule), the automation engine may generate a proactive suggestion or\\naction for the user. Meanwhile, the analytics component might produce a\\nhigher-level recommendation or alert based on patterns over time (for\\ninstance, an insight about the user’s habits). Both pathways lead to **user-\\nfacing outputs** – either through an AR interface, a notification, or another\\nUI – thereby closing the loop by delivering assistance or information back to\\nthe user. Finally, EEPIS incorporates a learning phase: the user’s feedback\\nand the outcome of actions feed into an AI model update process. The personal\\nAI model refines itself using these new data points (and potentially federated\\nupdates from other users, as described later), ensuring the system improves\\nover time. This continuous learning means each cycle makes EEPIS smarter and\\nmore attuned to the user, without compromising the user’s privacy\\u200b.\\n\\n![](images/image_9.png)\\n\\n### **Figure 9:High-Level EEPIS System Flowchart**\\n\\nThe figure above presents EEPIS’s operation as a **continuous loop of six\\nstages: Sense → Store → Analyze → Automate → Learn → Improve**. In the\\n**Sense** phase, the system perceives inputs from the user’s environment\\n(sensors capturing audio, video, biometrics, etc.). Next, in the **Store**\\nphase, important information from those inputs is securely stored into the\\nuser’s personal memory database (after filtering and processing). The\\n**Analyze** phase involves processing the stored data to understand context\\nand extract insights – for example, recognizing patterns, annotating context,\\nor correlating information. Following analysis, the system moves to\\n**Automate** , where it uses the insights and triggers to autonomously assist\\nthe user (such as suggesting an action or performing a routine task) subject\\nto user preferences. The outcomes of interactions then feed into the **Learn**\\nphase: EEPIS updates its AI models based on what happened – reinforcing\\nsuccessful behaviors and adjusting when the outcomes are not as desired.\\nFinally, the knowledge gained is used to **Improve** the system’s overall\\nperformance. This improvement directly feeds back into the next sensing phase\\n(dashed feedback loop), closing the cycle. Through repeated iterations of this\\nloop, EEPIS continually self-optimizes and evolves – effectively becoming more\\nintelligent and personalized with each pass\\u200b. This high-level loop underscores\\nthe system’s design as an ever-learning personal assistant that senses\\ncontext, remembers and learns from it, and proactively adapts to better serve\\nthe user over time.\\n\\n![](images/image_10.png)\\n\\n### **Figure 10: Multi-Agent/Device Collaboration**\\n\\nThe flowchart above depicts how multiple EEPIS instances (e.g. on a user’s\\nphone and laptop) collaborate to share tasks and data. On the left, _Device A_\\ncontinuously monitors context for any condition that triggers an automated\\ntask. If a task can be handled locally, Device A executes it immediately. If\\nnot, EEPIS checks whether it should **delegate the task to another device**\\n(perhaps for reasons of efficiency or availability). When delegation is needed\\n(branch “Yes”), Device A sends a task request to _Device B_ (right side).\\nDevice B receives the request, performs the task on behalf of Device A, and\\nreturns the result. Device A waits for this result, then integrates the\\noutcome and completes the task. Both devices also periodically sync data\\n(dotted line at bottom) to ensure their local memories and context are up-to-\\ndate. This multi-agent design enables a coordinated personal AI experience\\nacross a user’s ecosystem of devices – for example, two devices can negotiate\\nwhich one should handle a task for optimal efficiency, a capability not seen\\nin single-device assistants. The result is a seamless collaboration where\\nEEPIS instances work in unison, sharing updates and responsibilities while\\nrespecting the user’s privacy and intentions.\\n\\n![](images/image_11.png)\\n\\n### **Figure 11: User Control Mechanisms**\\n\\nThe diagram above details how EEPIS empowers the user to control the system’s\\nbehavior in three key areas: **privacy permissions, automation approvals, and\\ndata retention**. It all begins with the user accessing the EEPIS control\\npanel (settings interface). From there, the user can choose to manage Privacy\\nSettings, Automation Settings, or Data Management:\\n\\n  * **Privacy & Permissions** – The user can review and modify what data EEPIS is allowed to capture and retain. For example, they might enable/disable certain sensors or block recording in specific contexts. When the user updates these privacy settings, the system immediately applies the new rules to its Privacy Filter and data handling policies. This gives _granular user control over what is remembered or shared_ \\u200b  \\nfile-dxr2bcnxnvw4gfge5cwnpi  \\n. Going forward, any incoming data that violates the updated permissions will\\nbe filtered out, ensuring the user’s preferences are strictly enforced.  \\n\\n  * **Automation & Approval** – The user defines how autonomous the EEPIS assistant can be. In the settings, the user can specify which types of proactive actions require confirmation and which can execute automatically. For instance, the user may allow calendar scheduling fixes to happen without asking, but require confirmation before sending a message on their behalf. EEPIS updates its automation decision policies accordingly, so that during operation (as seen in the proactive automation flow), it will check these user-defined rules. The system always keeps the user in control by adhering to these preferences – only proceeding autonomously when permitted, otherwise seeking the user’s approval before acting.  \\n\\n  * **Data Retention & Deletion** – The user can manage their stored memories and set retention policies. Through the interface, they might select specific recorded events to delete or instruct the system to auto-delete certain data after a period of time. EEPIS will then carry out the requested deletions (erasing the data from all storage locations and indexes) or schedule the retention rules as directed. Every deletion is comprehensive, ensuring that no remnants remain – addressing the “right to be forgotten” by letting the user purge data completely at will\\u200b. The fact that users can edit or delete any personal data at any time is a distinguishing feature of EEPIS’s privacy-first approach.  \\n\\nOverall, this flowchart emphasizes that **user oversight is built into EEPIS**\\nat every level. Through an intuitive control panel, users actively govern\\ntheir data (what is stored or erased) and the AI’s autonomy (when it must\\npause for approval), which together ensure the system operates transparently\\nand in line with individual privacy and security preferences\\u200b\\n\\n![](images/image_12.png)\\n\\n### **Figure 12: Federated Learning & Model Updates**\\n\\nThe above diagram shows the federated learning cycle in EEPIS, which allows\\nthe AI model to improve continuously across all user devices **without\\ncentralizing raw personal data** \\u200b. The process starts on the user’s device\\nwith local model training: EEPIS takes the latest personal data (e.g. new\\nobservations, user interactions) and updates its AI models locally. Once this\\nlocal training phase is complete, the device computes a model update –\\nessentially the differences between the new model and the previous one – and\\nthen applies _differential privacy (DP) noise_ to that update\\u200b. This DP step\\nanonymizes the update, ensuring that it cannot reveal specific details of the\\nuser’s data. The anonymized model update is then uploaded to the EEPIS cloud\\naggregator. There, in the aggregation step, the server collects updates from\\nmany users’ devices (each update is privacy-protected and contains no raw user\\ndata) and combines them to build an improved global model\\u200b. For example, the\\nserver might average the weight changes from all devices to capture\\ngeneralizable patterns learned across the community. Once the global model is\\nrefined, the server distributes the updated global model back down to each\\nuser’s device. EEPIS on the device merges this global update into its local AI\\nmodel, **improving the personal assistant with collective knowledge** while\\nstill keeping personal details private. This cycle repeats periodically (e.g.\\nnightly or when the device is idle), meaning the AI is continuously learning\\nboth from the user’s own data and from anonymized insights of other users.\\nImportantly, throughout this federated learning process, the user’s raw data\\nnever leaves their device – only abstract model parameters are shared –\\npreserving privacy by design. In short, EEPIS’s federated learning flowchart\\ndemonstrates how the system gets progressively smarter for everyone, all the\\nwhile upholding each user’s data sovereignty.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a446e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f62275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(files) \n",
    "df[df['file_id']=='1lXqSeAg34kLJ-EX4v3hHhlNFy2a-SE27']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb6d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_iterator = DocumentTextIterator(d['access_token'], files)\n",
    "\n",
    "for title, content in doc_iterator:\n",
    "    if content:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4962a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999498b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    print(f['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3976bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
