{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7f7f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-10 16:22:26.433\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.ModelRunner\u001b[0m:\u001b[36mget_repo\u001b[0m:\u001b[36m88\u001b[0m - \u001b[34m\u001b[1m[[None]]] got repo for user None with roles [] and repo.role_level=None\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:26.452\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function help\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:26.739\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m64\u001b[0m - \u001b[34m\u001b[1madded function get_executive_info with access level 1\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:26.743\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function get_general_info\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:26.745\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function get_model_description\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:26.748\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m64\u001b[0m - \u001b[34m\u001b[1madded function get_partner_info with access level 10\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:26.752\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function get_recent_uploads_by_user\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:26.757\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function save_user_fact\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:26.759\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function search_user_facts\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:26.763\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function get_entities\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:26.766\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function search\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:26.768\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function activate_functions_by_name\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:26.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpercolate.services.ModelRunner\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m******Constructed agent p8.UserRoleAgent******\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:29.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpercolate.utils.env\u001b[0m:\u001b[36m_load_prompt\u001b[0m:\u001b[36m260\u001b[0m - \u001b[1mLoaded master prompt from database\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:29.960\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.llm.LanguageModel\u001b[0m:\u001b[36mcall_api_simple\u001b[0m:\u001b[36m322\u001b[0m - \u001b[34m\u001b[1minvoking model gpt-4.1, is_streaming=True\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 = 4\n",
      "\n",
      "If you have any questions about Resonance, Create-ONE, or need help with your products"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-10 16:22:31.210\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpercolate.services.llm.utils.stream_utils\u001b[0m:\u001b[36m_audit_response\u001b[0m:\u001b[36m726\u001b[0m - \u001b[1mAuditing stream response, content length: 126\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:31.211\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.llm.utils.stream_utils\u001b[0m:\u001b[36m_audit_response\u001b[0m:\u001b[36m733\u001b[0m - \u001b[34m\u001b[1mGenerated new session_id for audit: 0e51be82-190a-4c74-a62c-5f8039f4326c\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or orders, just let me know!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-10 16:22:32.349\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpercolate.services.llm.proxy.utils\u001b[0m:\u001b[36maudit_response_for_user\u001b[0m:\u001b[36m322\u001b[0m - \u001b[1mAudited session: 0e51be82-190a-4c74-a62c-5f8039f4326c for metadata {'userid': '10e0a97d-a064-553a-9043-3c1f0a6e6725', 'channel_id': None, 'thread_id': '0e51be82-190a-4c74-a62c-5f8039f4326c', 'query': 'what is 2 + 2'}\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:33.479\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpercolate.services.llm.proxy.utils\u001b[0m:\u001b[36maudit_response_for_user\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mUpdated user model for: 10e0a97d-a064-553a-9043-3c1f0a6e6725\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:34.593\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.llm.proxy.utils\u001b[0m:\u001b[36maudit_response_for_user\u001b[0m:\u001b[36m345\u001b[0m - \u001b[34m\u001b[1mAdded 1 AI Response records\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pytest\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "# Set environment variables BEFORE importing percolate\n",
    "os.environ[\"P8_PG_HOST\"] = \"localhost\"\n",
    "os.environ[\"P8_PG_PORT\"] = \"25432\"\n",
    "os.environ[\"P8_PG_USER\"] = \"app\"\n",
    "os.environ[\"P8_PG_PASSWORD\"] = os.environ.get(\"P8_TEST_BEARER_TOKEN\", \"\")\n",
    "os.environ[\"P8_PG_DATABASE\"] = \"app\"\n",
    "import percolate as p8\n",
    "from IPython.display import Markdown\n",
    "from percolate.services import PostgresService\n",
    "\n",
    "from percolate.api.mcp_server.api_repository import APIProxyRepository\n",
    "from percolate.services.llm import CallingContext\n",
    "from percolate.models import UserMemory\n",
    "from percolate.utils.env import MASTER_PROMPT\n",
    "from percolate.services.llm.utils.stream_utils import print_openai_delta_content\n",
    "#p8.repository(UserMemory).register()\n",
    "a = APIProxyRepository( user_email='amartey@gmail.com')\n",
    "pg = PostgresService()\n",
    "from percolate.models.p8 import UserRoleAgent\n",
    "#pg.execute('select id, name, email, role_level from  p8.\"User\" where role_level > 5')\n",
    "#a = APIProxyRepository( api_endpoint='http://localhost:5008', user_email='amartey@gmail.com')\n",
    "#await a.add_memory(\"I want to remember\")\n",
    "#await a.get_memory('unknown_20250728_112052_032')\n",
    "#Markdown(UserRoleAgent.get_model_description())\n",
    "#Markdown(MASTER_PROMPT())\n",
    "agent = p8.Agent(UserRoleAgent)\n",
    "context = CallingContext(user_id='10e0a97d-a064-553a-9043-3c1f0a6e6725', role_level=10)\n",
    "r = agent.stream(\"what is 2 + 2\",context=context)\n",
    "for item in r.iter_lines():\n",
    "    #print(item)\n",
    "    print_openai_delta_content(item.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cac04cf-fe60-4c6a-94be-85d08e086176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data: {\"id\":\"init\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\"},\"finish_reason\":null}]}\\n\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-10 16:22:36.739\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.llm.LanguageModel\u001b[0m:\u001b[36mcall_api_simple\u001b[0m:\u001b[36m322\u001b[0m - \u001b[34m\u001b[1minvoking model claude-3-7-sonnet-20250219, is_streaming=True\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:39.387\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mpercolate.services.llm.utils.stream_utils\u001b[0m:\u001b[36miter_lines\u001b[0m:\u001b[36m697\u001b[0m - \u001b[31m\u001b[1mError during streaming: 'choices'\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:39.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpercolate.services.llm.utils.stream_utils\u001b[0m:\u001b[36m_audit_response\u001b[0m:\u001b[36m726\u001b[0m - \u001b[1mAuditing stream response, content length: 0\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:39.389\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.llm.utils.stream_utils\u001b[0m:\u001b[36m_audit_response\u001b[0m:\u001b[36m733\u001b[0m - \u001b[34m\u001b[1mGenerated new session_id for audit: b5fb6992-7bd4-46c9-b4e3-550d849506c5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data: {\"id\":\"31b182ff-9b0e-4159-8dbf-6de86aa81159\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\\n\\n'\n",
      "b'data: [DONE]\\n\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-10 16:22:40.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpercolate.services.llm.proxy.utils\u001b[0m:\u001b[36maudit_response_for_user\u001b[0m:\u001b[36m322\u001b[0m - \u001b[1mAudited session: b5fb6992-7bd4-46c9-b4e3-550d849506c5 for metadata {'userid': '10e0a97d-a064-553a-9043-3c1f0a6e6725', 'channel_id': None, 'thread_id': 'b5fb6992-7bd4-46c9-b4e3-550d849506c5', 'query': 'what is 2 + 2'}\u001b[0m\n",
      "\u001b[32m2025-08-10 16:22:41.626\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpercolate.services.llm.proxy.utils\u001b[0m:\u001b[36maudit_response_for_user\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mUpdated user model for: 10e0a97d-a064-553a-9043-3c1f0a6e6725\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "context = CallingContext(user_id='10e0a97d-a064-553a-9043-3c1f0a6e6725', role_level=10, model='claude-3-7-sonnet-20250219')\n",
    "r = agent.stream(\"what is 2 + 2\",context=context)\n",
    "for item in r.iter_lines():\n",
    "    print_openai_delta_content(item.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00b5e763-c73b-49bf-a2be-ba5af76c8852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-10 16:28:15.460\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.llm.LanguageModel\u001b[0m:\u001b[36mcall_api_simple\u001b[0m:\u001b[36m322\u001b[0m - \u001b[34m\u001b[1minvoking model claude-3-7-sonnet-20250219, is_streaming=True\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     14\u001b[39m raw_response = lm_client._call_raw(\n\u001b[32m     15\u001b[39m         messages=M,\n\u001b[32m     16\u001b[39m         functions=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     17\u001b[39m         context=context,\n\u001b[32m     18\u001b[39m     )\n\u001b[32m     19\u001b[39m raw_response\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msse_openai_compatible_stream_with_tool_call_collapse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                \u001b[49m\u001b[43mraw_response\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/mr_saoirse/percolate/clients/python/percolate/percolate/services/llm/utils/stream_utils.py:437\u001b[39m, in \u001b[36msse_openai_compatible_stream_with_tool_call_collapse\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m line, chunk\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m choice = \u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchoices\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    438\u001b[39m delta = choice.get(\u001b[33m\"\u001b[39m\u001b[33mdelta\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m    439\u001b[39m finish_reason = choice.get(\u001b[33m\"\u001b[39m\u001b[33mfinish_reason\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'choices'"
     ]
    }
   ],
   "source": [
    "from percolate.services.llm import (\n",
    "    CallingContext,\n",
    "    FunctionCall,\n",
    "    LanguageModel,\n",
    "    MessageStackFormatter,\n",
    ")\n",
    "from percolate.services.llm.utils import (\n",
    "    sse_openai_compatible_stream_with_tool_call_collapse,\n",
    ")\n",
    "\n",
    "M = UserRoleAgent.build_message_stack(\"What is the captial of ireland\")\n",
    "context = CallingContext.with_model('claude-3-7-sonnet-20250219').in_streaming_mode()\n",
    "lm_client = LanguageModel.from_context(context)\n",
    "raw_response = lm_client._call_raw(\n",
    "        messages=M,\n",
    "        functions=None,\n",
    "        context=context,\n",
    "    )\n",
    "raw_response\n",
    "\n",
    "#internally uses response.iter_lines\n",
    "\n",
    "for line, chunk in sse_openai_compatible_stream_with_tool_call_collapse(\n",
    "                raw_response\n",
    "            ):\n",
    "        print(line, chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e80ab13-8815-458d-b063-027210b48e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-09 22:59:23.219\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.ModelRunner\u001b[0m:\u001b[36mget_repo\u001b[0m:\u001b[36m88\u001b[0m - \u001b[34m\u001b[1m[[None]]] got repo for user None with roles [] and repo.role_level=None\u001b[0m\n",
      "\u001b[32m2025-08-09 22:59:23.231\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function help\u001b[0m\n",
      "\u001b[32m2025-08-09 22:59:23.519\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m64\u001b[0m - \u001b[34m\u001b[1madded function get_executive_info with access level 1\u001b[0m\n",
      "\u001b[32m2025-08-09 22:59:23.522\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function get_general_info\u001b[0m\n",
      "\u001b[32m2025-08-09 22:59:23.523\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function get_model_description\u001b[0m\n",
      "\u001b[32m2025-08-09 22:59:23.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m64\u001b[0m - \u001b[34m\u001b[1madded function get_partner_info with access level 10\u001b[0m\n",
      "\u001b[32m2025-08-09 22:59:23.528\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function get_recent_uploads_by_user\u001b[0m\n",
      "\u001b[32m2025-08-09 22:59:23.531\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function save_user_memory\u001b[0m\n",
      "\u001b[32m2025-08-09 22:59:23.533\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function search_user_memories\u001b[0m\n",
      "\u001b[32m2025-08-09 22:59:23.535\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function get_entities\u001b[0m\n",
      "\u001b[32m2025-08-09 22:59:23.538\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function search\u001b[0m\n",
      "\u001b[32m2025-08-09 22:59:23.540\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function activate_functions_by_name\u001b[0m\n",
      "\u001b[32m2025-08-09 22:59:23.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpercolate.services.ModelRunner\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m******Constructed agent p8.UserRoleAgent******\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent = p8.Agent(UserRoleAgent)\n",
    "#agent.get_entities('KT-2011') #Box File: Cap Table 11.15.2023.xlsx\n",
    "#agent.get_entities('qa_circuits_memory.md') #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ed56f0-df0c-4b4c-946c-b3349658ecb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'get_entities': {'p8.Resources': {'data': [{'id': 'f52aafda-a8b8-5785-b46a-0e7f5a8e69fb',\n",
       "     'uri': 's3://res-data-platform/users/10e0a97d-a064-553a-9043-3c1f0a6e6725/Memory note about considering circuits for Q&A systems/qa_circuits_memory.md',\n",
       "     'name': 'qa_circuits_memory.md',\n",
       "     'userid': '10e0a97d-a064-553a-9043-3c1f0a6e6725',\n",
       "     'content': '# Memory: Consider Circuits for Q&A\\n\\n**Date:** July 27, 2025\\n**Priority:** Important\\n\\n## Note\\nNeed to consider the circuits for Q&A implementation.\\n\\n## Context\\n- This is a reminder to evaluate circuit design patterns for question and answer systems\\n- May relate to system architecture, data flow, or processing pathways\\n- Important consideration for future development or current project analysis\\n\\n## Action Items\\n- Review circuit requirements for Q&A functionality\\n- Analyze current circuit implementations\\n- Consider optimization opportunities\\n\\n---\\n*This memory was saved via Percolate MCP interface*',\n",
       "     'groupid': None,\n",
       "     'ordinal': 0,\n",
       "     'summary': None,\n",
       "     'user_id': None,\n",
       "     'category': 'text_chunk',\n",
       "     'metadata': {'file_type': 'text',\n",
       "      'chunk_size': 1000,\n",
       "      'chunk_index': 0,\n",
       "      'source_file': 'qa_circuits_memory.md',\n",
       "      'original_uri': 's3://res-data-platform/users/10e0a97d-a064-553a-9043-3c1f0a6e6725/Memory note about considering circuits for Q&A systems/qa_circuits_memory.md',\n",
       "      'parsing_mode': 'simple',\n",
       "      'total_chunks': 1,\n",
       "      'chunk_overlap': 200},\n",
       "     'created_at': '2025-07-27T22:05:39.22094',\n",
       "     'deleted_at': None,\n",
       "     'updated_at': '2025-08-09T18:40:24.611146',\n",
       "     'graph_paths': None,\n",
       "     'resource_timestamp': '2025-07-27T22:05:39.211257',\n",
       "     'required_access_level': 5}],\n",
       "   'metadata': None,\n",
       "   'instruction': 'you can request to activate new functions by name to use them as tools'}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pg.execute(\"\"\" select * from p8.\"User\" where email ='amartey@gmail.com' \"\"\")\n",
    "#UserRoleAgent.get_recent_uploads_by_user('10e0a97d-a064-553a-9043-3c1f0a6e6725')\n",
    "\n",
    "from percolate.services import PostgresService\n",
    "\n",
    "#does not work\n",
    "pg = PostgresService(user_id='10e0a97d-a064-553a-9043-3c1f0a6e6725')\n",
    "pg.get_entities('qa_circuits_memory.md')\n",
    "#works\n",
    "# pg = PostgresService()\n",
    "# pg.get_entities('qa_circuits_memory.md')\n",
    "# #works\n",
    "# pg = PostgresService(user_id='10e0a97d-a064-553a-9043-3c1f0a6e6725')\n",
    "# pg.get_entities('KT-2011')\n",
    "# #works\n",
    "# pg = PostgresService()\n",
    "# pg.get_entities('KT-2011')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6efa720b-0183-406e-8ef9-4d21430df17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-08-09 23:04:02.164\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.ModelRunner\u001b[0m:\u001b[36mget_repo\u001b[0m:\u001b[36m88\u001b[0m - \u001b[34m\u001b[1m[[None]]] got repo for user None with roles [] and repo.role_level=None\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:02.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function help\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:02.470\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m64\u001b[0m - \u001b[34m\u001b[1madded function get_executive_info with access level 1\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:02.474\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function get_general_info\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:02.476\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function get_model_description\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:02.479\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m64\u001b[0m - \u001b[34m\u001b[1madded function get_partner_info with access level 10\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:02.482\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function get_recent_uploads_by_user\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:02.486\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function save_user_memory\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:02.489\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function search_user_memories\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:02.493\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function get_entities\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:02.496\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function search\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:02.500\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.FunctionManager\u001b[0m:\u001b[36madd_function\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1madded function activate_functions_by_name\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:02.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpercolate.services.ModelRunner\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1m******Constructed agent p8.UserRoleAgent******\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:04.628\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpercolate.utils.env\u001b[0m:\u001b[36m_load_prompt\u001b[0m:\u001b[36m260\u001b[0m - \u001b[1mLoaded master prompt from database\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:04.629\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.llm.LanguageModel\u001b[0m:\u001b[36mcall_api_simple\u001b[0m:\u001b[36m320\u001b[0m - \u001b[34m\u001b[1minvoking model gpt-4.1, is_streaming=False\u001b[0m\n",
      "\u001b[32m2025-08-09 23:04:05.565\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpercolate.services.llm.LanguageModel\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m198\u001b[0m - \u001b[34m\u001b[1mresponse=<Response [200]>, context=CallingContext(session_id=None, chat_id=None, session_context=None, prefer_json=False, response_model=None, username=None, user_id=None, channel_context=None, channel_ts=None, prefers_streaming=False, is_hybrid_streaming=False, temperature=0.0, plan=None, max_iterations=5, model='gpt-4.1', file_uris=[], role_level=None, streaming_callback=None, response_callback=None)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "2 + 2 = 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "agent = p8.Agent(UserRoleAgent)\n",
    "r = agent(\"what is 2 + 2\")\n",
    "Markdown(r)\n",
    "context = CallingContext(user_id='10e0a97d-a064-553a-9043-3c1f0a6e6725', role_level=10)\n",
    "#r=agent('what functions do you have available - including the ones loaded as tools?', context=None)\n",
    "#r=agent('please search the executive resources for information about resonance patents', context=context)\n",
    "#r=agent('Can you tell me about KT-2011', context=context)\n",
    "#once db user is app this is restricted\n",
    "#r=agent('Can you tell me about KT-2011', context=CallingContext(role_level=10, user_id='3120a781-9c13-5dbd-8c18-9e72dc63f272'))\n",
    "\n",
    "#shoud return useful info about create one from synced files\n",
    "#r = agent(\"please give  me information about create one explaining what create one allows brands to do \")\n",
    "\n",
    "#r = agent(\"What recent files did i upload - load some chunks for the most interesting one\", context=context)\n",
    " \n",
    "Markdown(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb81cbdd-6d8b-4e8e-ba8d-8d7043f4f993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c924fb1-7bb6-4d27-8424-2afc6894e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = p8.try_load_model(\"public.CreateOneResources\", allow_abstract=True)\n",
    "repo = p8.repository(model)\n",
    "repo.search(\"What is Create ONE and what does it allow brands to do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6eef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = p8.try_load_model('design.bodies')\n",
    "loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from percolate.services import PostgresService\n",
    "from percolate.utils import make_uuid\n",
    "with open ('/Users/sirsh/code/mr_saoirse/percolate/.res/master_prompt.md') as f:\n",
    "    p = f.read()\n",
    "    settings_id = str(make_uuid({\"key\": \"system_prompt\"}))\n",
    "\n",
    "    query = '''\n",
    "    INSERT INTO p8.\"Settings\" (id, key, value, created_at, updated_at)\n",
    "    VALUES (%s, %s, %s, NOW(), NOW())\n",
    "    ON CONFLICT (id) DO UPDATE \n",
    "    SET value = EXCLUDED.value, updated_at = NOW()\n",
    "    '''\n",
    "    pg.execute(query, (settings_id, 'system_prompt', p))\n",
    "\n",
    "Markdown(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = p8.try_load_model('executive.ExecutiveResources')\n",
    "#p8.repository(loaded).search(\"Tell me about resonance companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc248e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54daa25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = p8.Agent(loaded).stream(\"tell me about resonance companies\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f4eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in r.iter_lines():\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfea910-cd35-4ec4-b161-cba90a2553a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from percolate.services import PostgresService\n",
    "\n",
    "#pg = PostgresService(user_id='3a9fb64e-8f26-511f-9ff3-09f0088d3b17')\n",
    "pg = PostgresService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478fe271-c015-4241-a79d-372d8b7a206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.execute(\"\"\" select * from executive.\"ExecutiveResources\" limit 1 \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4adbed-56c0-4b6b-9937-810ded7719d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "g = await a.stream_chat(\"tell me about the company\", stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#await a.get_entity('kt2011')\n",
    "r = await a.search_entities(\"Looking for entities learning about bodies\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1df098",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = await a.search_entities(\"Im looking for summer dresses\",entity_name='design.bodies')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9196f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r[0].keys())\n",
    "r[0]['vector_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b973a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = await a.stream_chat(\"tell me about the resonance company\", stream=False)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6795ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = await a.stream_chat(\"tell me about the resonance company\", stream=False, agent='executive-ExecutiveResources')\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5044a-b339-4dee-a90e-0e02a4eab6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pg.execute(\"select body_code from design.bodies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee38590-334e-45cf-845f-539e4c9ec6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pg.get_entities('KT-2011'), pg.get_entities('ZF-3005')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf41aa8-e390-4275-b0a5-0972bcb9849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from percolate.services.llm import CallingContext\n",
    "import percolate as p8\n",
    "from percolate.models import Resources\n",
    "\n",
    "ctx = CallingContext(user_id='3a9fb64e-8f26-511f-9ff3-09f0088d3b17')\n",
    "a = p8.Agent(Resources,allow_help=False)\n",
    "#a.get_entities('ZF-3005')\n",
    "a(\"get me KT-2011\", context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37103b41-5193-4b69-89ae-29230fa3b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a(\"get me ZF-3005\", context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea6ccb8-4c94-4ee3-8db9-fb5aa54bfd30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085421d-7e23-4f04-bb2e-743917070c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
