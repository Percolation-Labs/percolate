#!/usr/bin/env python3
"""
Test script to check LLMStreamIterator behavior with OpenWebUI
focusing on stream ending, [DONE] signal, and headers.
"""

import os
import json
import time
import logging
import requests
from datetime import datetime
import argparse

# Enable verbose logging
logging.basicConfig(level=logging.DEBUG, 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def run_percolate_test(model="gpt-4.1-mini", token="postgres"):
    """
    Test the actual Percolate streaming implementation
    """
    url = "http://localhost:5000/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {token}"
    }
    
    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": "Write a brief paragraph about streaming APIs."}
        ],
        "stream": True
    }
    
    logger.info(f"Testing Percolate streaming with model {model}")
    start_time = datetime.now()
    
    try:
        response = requests.post(url, headers=headers, json=data, stream=True)
        
        if response.status_code != 200:
            logger.error(f"Error: Received status code {response.status_code}")
            logger.error(f"Response: {response.text}")
            return
        
        logger.info(f"Response headers: {response.headers}")
        
        # Check for essential streaming headers
        essential_headers = {
            'Content-Type': 'text/event-stream',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'X-Accel-Buffering': 'no'
        }
        
        for header, expected_value in essential_headers.items():
            header_lower = header.lower()
            if header_lower in response.headers:
                value = response.headers[header_lower]
                if expected_value in value:
                    logger.info(f"✓ {header}: {value}")
                else:
                    logger.warning(f"✗ {header}: Expected '{expected_value}', got '{value}'")
            else:
                logger.warning(f"✗ Missing header: {header}")
        
        content = ""
        status_seen = False
        done_marker_seen = False
        finish_reason_seen = False
        
        # Process the stream
        for line in response.iter_lines(decode_unicode=True):
            if not line or not line.startswith("data: "):
                continue
            
            raw_data = line[6:].strip()
            
            if raw_data == "[DONE]":
                logger.info("[DONE] marker received")
                done_marker_seen = True
                break
            
            try:
                data = json.loads(raw_data)
                
                # Check for status message
                if "event" in data and data["event"] == "status":
                    logger.info(f"Status message: {data['message']}")
                    status_seen = True
                    continue
                
                # Extract content
                if "choices" in data and data["choices"]:
                    choice = data["choices"][0]
                    delta = choice.get("delta", {})
                    
                    if "content" in delta:
                        content_chunk = delta["content"]
                        content += content_chunk
                        print(content_chunk, end="", flush=True)
                    
                    # Log finish reason
                    if choice.get("finish_reason"):
                        logger.info(f"Finish reason: {choice['finish_reason']}")
                        finish_reason_seen = True
                        
                # Check for usage data
                if "usage" in data:
                    logger.info(f"Usage data received: {data['usage']}")
                    
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse: {raw_data}")
        
        print("\n")
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # Report on findings
        logger.info("\nSummary:")
        logger.info(f"- Status message: {'Yes' if status_seen else 'No'}")
        logger.info(f"- [DONE] marker: {'Yes' if done_marker_seen else 'No'}")
        logger.info(f"- Finish reason: {'Yes' if finish_reason_seen else 'No'}")
        logger.info(f"- Content length: {len(content)} characters")
        logger.info(f"- Total time: {duration:.2f} seconds")
        
        # If no [DONE] marker was seen, this could be why OpenWebUI hangs
        if not done_marker_seen:
            logger.error("No [DONE] marker received - this could cause OpenWebUI to hang")
            
        # If no finish reason was seen, this is also problematic
        if not finish_reason_seen:
            logger.warning("No finish_reason seen in the response - clients may not know when message is complete")
            
    except Exception as e:
        logger.error(f"Error during test: {str(e)}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Test LLM streaming behavior")
    parser.add_argument("--model", default="gpt-4.1-mini", help="Model to use for Percolate test")
    parser.add_argument("--token", default="postgres", help="Bearer token for Percolate test")
    
    args = parser.parse_args()
    run_percolate_test(args.model, args.token)